{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R94pjX_-bQHa"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "  tpu_worker = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466') \n",
        "  print(tf.profiler.experimental.client.monitor(tpu_worker,1))\n",
        "except ValueError:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  #raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asp1UpAHW4Ol",
        "outputId": "88cbc161-0d1c-471f-a650-d80a94c729fd"
      },
      "outputs": [],
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip install opencv-python==4.5.5.64\n",
        "!pip install opencv-contrib-python\n",
        "!pip install tensorflow_addons\n",
        "!pip install vit-keras\n",
        "!pip install tf-models-official==2.9.2\n",
        "!pip install -q -U keras-tuner\n",
        "!pip install focal-loss\n",
        "#!pip uninstall tensorflow\n",
        "#!pip install tensorflow==2.4.1\n",
        "#!pip uninstall tensorflow tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Cu8VzAbdN2",
        "outputId": "144b5779-19fc-44d0-d94a-4b0464c11440"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'stiarnet-v2'\n",
        "\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xSI8sKr4_qj",
        "outputId": "6fc91d66-f1ff-463d-8e3b-41fcb8cb41c4"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -al gs://"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j-Ut9Y_UbaM",
        "outputId": "7780b4c1-824e-4b66-d529-7d37c6fcaf93"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQH6D-NAz5I2",
        "outputId": "04143535-6fc1-446a-ecc3-ac51354756e6"
      },
      "outputs": [],
      "source": [
        "!pip uninstall tensorflow --yes\n",
        "!pip install tensorflow==2.8.1\n",
        "!pip install keras-flops\n",
        "!pip install mock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4ofSOy3UsP2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import ast\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from tqdm import trange\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "import typing_extensions as tx\n",
        "import matplotlib.pyplot as plt\n",
        "from vit_keras import vit, utils\n",
        "from keras_flops import get_flops\n",
        "from sklearn.metrics import accuracy_score\n",
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "from official.projects.movinet.modeling import movinet\n",
        "#from official.projects.movinet.modeling import movinet_model\n",
        "from official.vision.modeling.layers import nn_layers\n",
        "from official.projects.movinet.modeling.movinet_layers import ConvBlock, Squeeze3D\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "from official.vision.configs import video_classification\n",
        "from official.projects.movinet.configs import movinet as movinet_configs\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_layers\n",
        "from official.projects.movinet.modeling import movinet_model\n",
        "from official.projects.movinet.tools import export_saved_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "bBpAGNLfs_pA",
        "outputId": "0107cab6-9623-4d45-ddaa-16db4154d22a"
      },
      "outputs": [],
      "source": [
        "tf.__version__, np.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fuE4SB0ZzrQ"
      },
      "outputs": [],
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CigmXccqUt7I",
        "outputId": "21c1be24-bb95-47b4-edc0-6f7b8ebfa6af"
      },
      "outputs": [],
      "source": [
        "# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU/GPU/multi-GPU/cluster-GPU detection code\n",
        "\n",
        "try: # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: # detect GPUs\n",
        "    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n",
        "\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5anyDbG4pzCi",
        "outputId": "798dfa9e-8e0c-4dea-d955-4c21eab382a1"
      },
      "outputs": [],
      "source": [
        "strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsFknIAYynfs"
      },
      "source": [
        "# FLAGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNh-oEmHmorI",
        "outputId": "89872a62-d6f8-49ca-d174-f8fc922fc405"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "CLASS_MAP = {'IS': 0 , 'ISLG': 1, 'LG': 2, 'LGIS': 3}       \n",
        "\n",
        "GCS_PATH = 'gs://stairnet-v2/StairNet_Video_splits/'\n",
        "print(GCS_PATH)\n",
        "\n",
        "\n",
        "TRAIN_FILENAMES = tf.io.gfile.glob(GCS_PATH + 'train_*.tfrecord')\n",
        "print(TRAIN_FILENAMES)\n",
        "VAL_FILENAMES = tf.io.gfile.glob(GCS_PATH + 'val_*.tfrecord')\n",
        "print(VAL_FILENAMES)\n",
        "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + 'test_*.tfrecord') # predictions on this dataset should be submitted for the competition\n",
        "print(TEST_FILENAMES)\n",
        "\n",
        "\n",
        "# set the number of epochs for the run                        \n",
        "EPOCHS = 10\n",
        "\n",
        "# set the initial learning rate\n",
        "BASE_LR = 0.00001\n",
        "\n",
        "# 8 TPU cores, so 16 will be 128\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "BATCH_SIZE_VAL = 32 * strategy.num_replicas_in_sync\n",
        "\n",
        "#To study the effect of transfer training:\n",
        "FINE_TUNE_BUFFER = -1\n",
        "\n",
        "# Buffer size of the dataset\n",
        "BUFFER_SIZE = 100\n",
        "\n",
        "#Change Droprate \n",
        "DROPOUT_VALUE = 0.2\n",
        "\n",
        "# For hyperparameter optimization set to 144 \n",
        "# For final run set to 'None'\n",
        "SEED_NUMBER = 144 \n",
        "\n",
        "SEQ_LEN = 5\n",
        "INIT_IMAGE_SIZE = 256\n",
        "IMAGE_CROP_SIZE = 256\n",
        "NUM_CHANNELS = 3\n",
        "\n",
        "ENCODER_NAME = 'efficient_b0' # ['movinet', 'mobilevit', 'vit', 'vgg', 'mobilenetv2', 'efficient_b0']\n",
        "TEMPORAL_MODEL = 'lstm' # ['transformer', 'lstm']\n",
        "MANY2ONE = True\n",
        "\n",
        "OPTIMIZER = 'adam'\n",
        "\n",
        "EVALUATE_INFERENCE_SPEED = False\n",
        "CHECK_UPSAMPLE_DISTRBUTION = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "caJkbwcd1wqe",
        "outputId": "9750b930-5bed-40c2-a7d5-8eed511333e6"
      },
      "outputs": [],
      "source": [
        "len(TRAIN_FILENAMES), len(VAL_FILENAMES), len(TEST_FILENAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6xVBoH3yxy8"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_c7TBECrzv7"
      },
      "outputs": [],
      "source": [
        "def decode_image_seq(image_data):\n",
        "    ''' reading sequence from bytes array '''\n",
        "    image = tf.io.decode_raw(image_data, 'float64')\n",
        "    image = tf.reshape(image, [SEQ_LEN, INIT_IMAGE_SIZE, INIT_IMAGE_SIZE, NUM_CHANNELS]) # explicit size needed for TPU\n",
        "    image = tf.cast(image, tf.float32) / 255.\n",
        "    image = tf.image.random_crop(value=image, size=(SEQ_LEN, IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS))\n",
        "    return image\n",
        "\n",
        "def convert_str_label(tf_id):\n",
        "    ''' casting bytes to integer label '''\n",
        "    _id = tf_id.numpy().decode('utf-8')\n",
        "    _id = ast.literal_eval(_id)\n",
        "    labels = [CLASS_MAP[el] for el in _id][-1]\n",
        "    return tf.cast(labels, tf.int32)\n",
        "\n",
        "def read_labeled_tfrecord(example):\n",
        "    ''' reading sample from tfrecord '''\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    image_seq = decode_image_seq(example['image'])\n",
        "    label = tf.io.decode_raw(example['label'], 'int32')[-2]\n",
        "    return image_seq, label # returns a dataset of image(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AylPvGAZ2PC"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filenames, ordered=False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
        "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEIC1I5EU2Uv"
      },
      "outputs": [],
      "source": [
        "def get_validation_dataset(repeated=False, ordered=False):\n",
        "    dataset = load_dataset(VAL_FILENAMES)\n",
        "    if repeated:\n",
        "      dataset = dataset.repeat(EPOCHS)\n",
        "      # for hyperparameter testing using random seed to shuffle the data the same to elimite this variable from results\n",
        "      dataset = dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED_NUMBER, reshuffle_each_iteration=None)\n",
        "    dataset = dataset.batch(BATCH_SIZE_VAL, drop_remainder=repeated)\n",
        "    #dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "    return dataset\n",
        "\n",
        "def get_train_dataset():\n",
        "    dataset = load_dataset(TRAIN_FILENAMES)\n",
        "    dataset = dataset.repeat()\n",
        "    # for hyperparameter testing using random seed to shuffle the data the same to elimite this variable from results\n",
        "    dataset = dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED_NUMBER, reshuffle_each_iteration=None)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    #dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "    return dataset\n",
        "\n",
        "def get_test_dataset():\n",
        "    dataset = load_dataset(TEST_FILENAMES)\n",
        "    dataset = dataset.batch(BATCH_SIZE_VAL)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "debZaxD8xfmf"
      },
      "outputs": [],
      "source": [
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FSrVI2lxdrf"
      },
      "outputs": [],
      "source": [
        "NUM_TRAINING_IMAGES = 426177 \n",
        "NUM_VALIDATION_IMAGES = 32487 \n",
        "NUM_TEST_IMAGES = 56729\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "VALIDATION_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "TEST_STEPS = NUM_TEST_IMAGES // BATCH_SIZE            # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "print('Dataset: {} training images, {} validation images, {} test images'.format(\n",
        "    NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))\n",
        "print('Dataset: {} training steps, {} validation steps, {} test steps'.format(\n",
        "    STEPS_PER_EPOCH, VALIDATION_STEPS, TEST_STEPS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqDtCh8q00v6"
      },
      "outputs": [],
      "source": [
        "# Get labels and their countings\n",
        "from collections import Counter\n",
        "\n",
        "def get_training_dataset_raw():\n",
        "    dataset = load_dataset(TRAIN_FILENAMES, ordered=False)\n",
        "    return dataset\n",
        "\n",
        "raw_training_dataset = get_training_dataset_raw() # default dataset \n",
        "\n",
        "label_counter = Counter()\n",
        "for images, labels in raw_training_dataset:\n",
        "    label_counter.update([labels.numpy()])\n",
        "\n",
        "del raw_training_dataset    \n",
        "    \n",
        "label_counting_sorted = label_counter.most_common()\n",
        "\n",
        "NUM_TRAINING_IMAGES = sum([x[1] for x in label_counting_sorted])\n",
        "print(\"number of examples in the original training dataset: {}\".format(NUM_TRAINING_IMAGES))\n",
        "\n",
        "print(\"labels in the original training dataset, sorted by occurrence\")\n",
        "print(label_counting_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc-Wpc7R0SUG"
      },
      "outputs": [],
      "source": [
        "# We want each class occur at least (approximately) `TARGET_MIN_COUNTING` times\n",
        "\n",
        "TARGET_MIN_COUNTING = 300000\n",
        "\n",
        "def get_num_of_repetition_for_class(class_id):\n",
        "    counting = label_counter[class_id]\n",
        "    if counting >= TARGET_MIN_COUNTING:\n",
        "        return 1.0\n",
        "    num_to_repeat = TARGET_MIN_COUNTING / counting\n",
        "    return num_to_repeat\n",
        "\n",
        "numbers_of_repetition_for_classes = {class_id: get_num_of_repetition_for_class(class_id) for class_id in range(4)}\n",
        "\n",
        "print(\"number of repetitions for each class (if > 1)\")\n",
        "{k: v for k, v in sorted(numbers_of_repetition_for_classes.items(), key=lambda item: item[1], reverse=True) if v > 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zLYAlPpOXgl"
      },
      "outputs": [],
      "source": [
        "keys_tensor = tf.constant([k for k in numbers_of_repetition_for_classes])\n",
        "vals_tensor = tf.constant([numbers_of_repetition_for_classes[k] for k in numbers_of_repetition_for_classes])\n",
        "table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), -1)\n",
        "\n",
        "def get_num_of_repetition_for_example(train_sample):\n",
        "  ''' counting number of samples that share the same label '''\n",
        "  _, label  = train_sample\n",
        "  num_to_repeat = table.lookup(label)\n",
        "  num_to_repeat_integral = tf.cast(int(num_to_repeat), tf.float32)\n",
        "  residue = num_to_repeat - num_to_repeat_integral\n",
        "  num_to_repeat = num_to_repeat_integral + tf.cast(tf.random.uniform(shape=()) <= residue, tf.float32)\n",
        "  return tf.cast(num_to_repeat, tf.int64)\n",
        "\n",
        "def get_train_dataset_with_oversample(oversample=False):\n",
        "  ''' costructing new dataset with class oversampling '''\n",
        "  dataset = load_dataset(TRAIN_FILENAMES)\n",
        "\n",
        "  if oversample:\n",
        "    dataset = dataset.flat_map(\n",
        "        lambda sequence, label: tf.data.Dataset.from_tensors((sequence, label)).repeat(\n",
        "            get_num_of_repetition_for_example((sequence, label))\n",
        "        )\n",
        "    )\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.shuffle(20000)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "  dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpN1O2JkRB7z"
      },
      "outputs": [],
      "source": [
        "# Get labels and their countings\n",
        "\n",
        "if CHECK_UPSAMPLE_DISTRBUTION:\n",
        "  def get_training_dataset_raw(oversample=True):\n",
        "    dataset = load_dataset(TRAIN_FILENAMES)\n",
        "    if oversample:\n",
        "      dataset = dataset.flat_map(\n",
        "        lambda sequence, label: tf.data.Dataset.from_tensors((sequence, label)).repeat(\n",
        "            get_num_of_repetition_for_example((sequence, label))\n",
        "        )\n",
        "      )\n",
        "    return dataset\n",
        "\n",
        "  raw_training_dataset = get_training_dataset_raw()\n",
        "\n",
        "  label_counter = Counter()\n",
        "  for images, labels in raw_training_dataset:\n",
        "    label_counter.update([labels.numpy()])\n",
        "\n",
        "  del raw_training_dataset    \n",
        "    \n",
        "  label_counting_sorted = label_counter.most_common()\n",
        "\n",
        "  NUM_TRAINING_IMAGES = sum([x[1] for x in label_counting_sorted])\n",
        "  print(\"number of examples in the original training dataset: {}\".format(NUM_TRAINING_IMAGES))\n",
        "\n",
        "  print(\"labels in the original training dataset, sorted by occurrence\")\n",
        "  print(label_counting_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGZItKdGU2my"
      },
      "source": [
        "# Model (Encoders)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNxy7ycJqtRE"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://github.com/faustomorales/vit-keras/releases/download/dl\"\n",
        "WEIGHTS = {\"imagenet21k\": 21_843, \"imagenet21k+imagenet2012\": 1_000}\n",
        "\n",
        "ConfigDict = tx.TypedDict(\n",
        "    \"ConfigDict\",\n",
        "    {\n",
        "        \"dropout\": float,\n",
        "        \"mlp_dim\": int,\n",
        "        \"num_heads\": int,\n",
        "        \"num_layers\": int,\n",
        "        \"hidden_size\": int,\n",
        "    },\n",
        ")\n",
        "\n",
        "CONFIG_B: ConfigDict = {\n",
        "    \"dropout\": 0.1,\n",
        "    \"mlp_dim\": 3072,\n",
        "    \"num_heads\": 12,\n",
        "    \"num_layers\": 12,\n",
        "    \"hidden_size\": 768,\n",
        "}\n",
        "\n",
        "class AddPositionEmbs(tf.keras.layers.Layer):\n",
        "    \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert (\n",
        "            len(input_shape) == 3\n",
        "        ), f\"Number of dimensions should be 3, got {len(input_shape)}\"\n",
        "        self.pe = tf.Variable(\n",
        "            name=\"pos_embedding\",\n",
        "            initial_value=tf.random_normal_initializer(stddev=0.06)(\n",
        "                shape=(1, input_shape[1], input_shape[2])\n",
        "            ),\n",
        "            dtype=\"float32\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[2])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + tf.cast(self.pe, dtype=inputs.dtype)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *args, num_heads, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        hidden_size = input_shape[-1]\n",
        "        num_heads = self.num_heads\n",
        "        if hidden_size % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {hidden_size} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.hidden_size = hidden_size\n",
        "        self.projection_dim = hidden_size // num_heads\n",
        "        self.query_dense = tf.keras.layers.Dense(hidden_size, name=\"query\")\n",
        "        self.key_dense = tf.keras.layers.Dense(hidden_size, name=\"key\")\n",
        "        self.value_dense = tf.keras.layers.Dense(hidden_size, name=\"value\")\n",
        "        self.combine_heads = tf.keras.layers.Dense(hidden_size, name=\"out\")\n",
        "\n",
        "    # pylint: disable=no-self-use\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], score.dtype)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.hidden_size))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output, weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_heads\": self.num_heads})\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"Implements a Transformer block.\"\"\"\n",
        "\n",
        "    def __init__(self, *args, num_heads, mlp_dim, dropout, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.att = MultiHeadSelfAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            name=\"MultiHeadDotProductAttention_1\",\n",
        "        )\n",
        "        self.mlpblock = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.Dense(\n",
        "                    self.mlp_dim,\n",
        "                    activation=\"linear\",\n",
        "                    name=f\"{self.name}/Dense_0\",\n",
        "                ),\n",
        "                tf.keras.layers.Lambda(\n",
        "                    lambda x: tf.keras.activations.gelu(x, approximate=False)\n",
        "                )\n",
        "                if hasattr(tf.keras.activations, \"gelu\")\n",
        "                else tf.keras.layers.Lambda(\n",
        "                    lambda x: tf.activations.gelu(x, approximate=False)\n",
        "                ),\n",
        "                tf.keras.layers.Dropout(self.dropout),\n",
        "                tf.keras.layers.Dense(input_shape[-1], name=f\"{self.name}/Dense_1\"),\n",
        "                tf.keras.layers.Dropout(self.dropout),\n",
        "            ],\n",
        "            name=\"MlpBlock_3\",\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6, name=\"LayerNorm_0\"\n",
        "        )\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6, name=\"LayerNorm_2\"\n",
        "        )\n",
        "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.layernorm1(inputs)\n",
        "        x, weights = self.att(x)\n",
        "        x = self.dropout_layer(x, training=training)\n",
        "        x = x + inputs\n",
        "        y = self.layernorm2(x)\n",
        "        y = self.mlpblock(y)\n",
        "        return x + y, weights\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[2])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"mlp_dim\": self.mlp_dim,\n",
        "                \"dropout\": self.dropout,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "class ClassToken(tf.keras.layers.Layer):\n",
        "    \"\"\"Append a class token to an input layer.\"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        cls_init = tf.zeros_initializer()\n",
        "        self.hidden_size = input_shape[-1]\n",
        "        self.cls = tf.Variable(\n",
        "            name=\"cls\",\n",
        "            initial_value=cls_init(shape=(1, 1, self.hidden_size), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1] + 1, input_shape[2])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        cls_broadcasted = tf.cast(\n",
        "            tf.broadcast_to(self.cls, [batch_size, 1, self.hidden_size]),\n",
        "            dtype=inputs.dtype,\n",
        "        )\n",
        "        return tf.concat([cls_broadcasted, inputs], 1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "class SliceClassToken(tf.keras.layers.Layer):\n",
        "    \"\"\"Append a class token to an input layer.\"\"\"\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1] - 1, input_shape[2])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs[:, :-1, :]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "def interpret_image_size(image_size_arg):\n",
        "    \"\"\"Process the image_size argument whether a tuple or int.\"\"\"\n",
        "    if isinstance(image_size_arg, int):\n",
        "        return (image_size_arg, image_size_arg)\n",
        "    if (\n",
        "        isinstance(image_size_arg, tuple)\n",
        "        and len(image_size_arg) == 2\n",
        "        and all(map(lambda v: isinstance(v, int), image_size_arg))\n",
        "    ):\n",
        "        return image_size_arg\n",
        "    raise ValueError(\n",
        "        f\"The image_size argument must be a tuple of 2 integers or a single integer. Received: {image_size_arg}\"\n",
        "    )\n",
        "\n",
        "def build_model(\n",
        "    image_size,\n",
        "    patch_size: int,\n",
        "    num_layers: int,\n",
        "    hidden_size: int,\n",
        "    num_heads: int,\n",
        "    name: str,\n",
        "    mlp_dim: int,\n",
        "    classes: int,\n",
        "    dropout=0.1,\n",
        "    activation=\"linear\",\n",
        "    include_top=True,\n",
        "    representation_size=None,\n",
        "):\n",
        "    \"\"\"Build a ViT model.\n",
        "    Args:\n",
        "        image_size: The size of input images.\n",
        "        patch_size: The size of each patch (must fit evenly in image_size)\n",
        "        classes: optional number of classes to classify images\n",
        "            into, only to be specified if `include_top` is True, and\n",
        "            if no `weights` argument is specified.\n",
        "        num_layers: The number of transformer layers to use.\n",
        "        hidden_size: The number of filters to use\n",
        "        num_heads: The number of transformer heads\n",
        "        mlp_dim: The number of dimensions for the MLP output in the transformers.\n",
        "        dropout_rate: fraction of the units to drop for dense layers.\n",
        "        activation: The activation to use for the final layer.\n",
        "        include_top: Whether to include the final classification layer. If not,\n",
        "            the output will have dimensions (batch_size, hidden_size).\n",
        "        representation_size: The size of the representation prior to the\n",
        "            classification layer. If None, no Dense layer is inserted.\n",
        "    \"\"\"\n",
        "    image_size_tuple = interpret_image_size(image_size)\n",
        "    assert (image_size_tuple[0] % patch_size == 0) and (\n",
        "        image_size_tuple[1] % patch_size == 0\n",
        "    ), \"image_size must be a multiple of patch_size\"\n",
        "    x = tf.keras.layers.Input(shape=(image_size_tuple[0], image_size_tuple[1], 3))\n",
        "    y = tf.keras.layers.Conv2D(\n",
        "        filters=hidden_size,\n",
        "        kernel_size=patch_size,\n",
        "        strides=patch_size,\n",
        "        padding=\"valid\",\n",
        "        name=\"embedding\",\n",
        "    )(x)\n",
        "    y = tf.keras.layers.Reshape((y.shape[1] * y.shape[2], hidden_size))(y)\n",
        "    y = ClassToken(name=\"class_token\")(y)\n",
        "    y = AddPositionEmbs(name=\"Transformer/posembed_input\")(y)\n",
        "    for n in range(num_layers):\n",
        "        y, _ = TransformerBlock(\n",
        "            num_heads=num_heads,\n",
        "            mlp_dim=mlp_dim,\n",
        "            dropout=dropout,\n",
        "            name=f\"Transformer/encoderblock_{n}\",\n",
        "        )(y)\n",
        "    y = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"Transformer/encoder_norm\"\n",
        "    )(y)\n",
        "    y = tf.keras.layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(y)\n",
        "    if representation_size is not None:\n",
        "        y = tf.keras.layers.Dense(\n",
        "            representation_size, name=\"pre_logits\", activation=\"tanh\"\n",
        "        )(y)\n",
        "    if include_top:\n",
        "        y = tf.keras.layers.Dense(classes, name=\"head\", activation=activation)(y)\n",
        "    return tf.keras.models.Model(inputs=x, outputs=y, name=name)\n",
        "\n",
        "\n",
        "def validate_pretrained_top(\n",
        "    include_top: bool, pretrained: bool, classes: int, weights: str\n",
        "):\n",
        "    \"\"\"Validate that the pretrained weight configuration makes sense.\"\"\"\n",
        "    assert weights in WEIGHTS, f\"Unexpected weights: {weights}.\"\n",
        "    expected_classes = WEIGHTS[weights]\n",
        "    assert include_top, \"Can only use pretrained_top with include_top.\"\n",
        "    assert pretrained, \"Can only use pretrained_top with pretrained.\"\n",
        "    return expected_classes\n",
        "\n",
        "\n",
        "def load_pretrained(\n",
        "    size: str,\n",
        "    weights: str,\n",
        "    pretrained_top: bool,\n",
        "    model: tf.keras.models.Model,\n",
        "    image_size,\n",
        "    patch_size: int,\n",
        "):\n",
        "    \"\"\"Load model weights for a known configuration.\"\"\"\n",
        "    image_size_tuple = interpret_image_size(image_size)\n",
        "    fname = f\"ViT-{size}_{weights}.npz\"\n",
        "    origin = f\"{BASE_URL}/{fname}\"\n",
        "    local_filepath = tf.keras.utils.get_file(fname, origin, cache_subdir=\"weights\")\n",
        "    utils.load_weights_numpy(\n",
        "        model=model,\n",
        "        params_path=local_filepath,\n",
        "        pretrained_top=pretrained_top,\n",
        "        num_x_patches=image_size_tuple[1] // patch_size,\n",
        "        num_y_patches=image_size_tuple[0] // patch_size,\n",
        "    )\n",
        "\n",
        "\n",
        "def vit_b16(\n",
        "    image_size = (224, 224),\n",
        "    classes=1000,\n",
        "    activation=\"linear\",\n",
        "    include_top=True,\n",
        "    pretrained=True,\n",
        "    pretrained_top=True,\n",
        "    weights=\"imagenet21k+imagenet2012\",\n",
        "):\n",
        "    \"\"\"Build ViT-B16. All arguments passed to build_model.\"\"\"\n",
        "    if pretrained_top:\n",
        "        classes = validate_pretrained_top(\n",
        "            include_top=include_top,\n",
        "            pretrained=pretrained,\n",
        "            classes=classes,\n",
        "            weights=weights,\n",
        "        )\n",
        "    model = build_model(\n",
        "        **CONFIG_B,\n",
        "        name=\"vit-b16\",\n",
        "        patch_size=16,\n",
        "        image_size=image_size,\n",
        "        classes=classes,\n",
        "        activation=activation,\n",
        "        include_top=include_top,\n",
        "        representation_size=768 if weights == \"imagenet21k\" else None,\n",
        "    )\n",
        "\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            size=\"B_16\",\n",
        "            weights=weights,\n",
        "            model=model,\n",
        "            pretrained_top=pretrained_top,\n",
        "            image_size=image_size,\n",
        "            patch_size=16,\n",
        "        )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKSD1ELaXk7o"
      },
      "outputs": [],
      "source": [
        "# Visual Transfromer model check\n",
        "\n",
        "#image_size = 224\n",
        "#base_model = vit_b16(\n",
        "#    image_size=image_size,\n",
        "#    activation='sigmoid',\n",
        "#    pretrained=True,\n",
        "#    include_top=False,\n",
        "#    pretrained_top=False,\n",
        "#)\n",
        "#x = base_model.layers[-2].output\n",
        "#x = SliceClassToken()(x) \n",
        "#patch_size = int(math.sqrt(x.shape[-2]))\n",
        "#embedding_dim = x.shape[-1]\n",
        "#embedding = tf.keras.layers.Reshape((\n",
        "#    patch_size, patch_size, \n",
        "#    embedding_dim))(x)\n",
        "#base_model = tf.keras.Model(inputs=base_model.input, outputs=embedding)\n",
        "#base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MobileViT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR4vbRIBLAou"
      },
      "outputs": [],
      "source": [
        "class InvertedRes(tf.keras.layers.Layer):\n",
        "  def __init__(self, expand_channels, output_channels, strides=1):\n",
        "    super().__init__()\n",
        "    self.output_channels = output_channels\n",
        "    self.strides = strides\n",
        "    self.expand_channels = expand_channels\n",
        "    self.expand = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(expand_channels, 1, padding=\"same\", use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('swish')\n",
        "                                        ], name=\"expand\")\n",
        "    self.dw_conv = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.DepthwiseConv2D(3, strides=strides, padding=\"same\", use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('swish')\n",
        "                                        ], name=\"depthwise\")\n",
        "    self.pw_conv = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "                                        ], name='pointwise')\n",
        "  \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (\n",
        "      input_shape[0], \n",
        "      input_shape[1] // self.strides, \n",
        "      input_shape[2]  // self.strides, \n",
        "      self.output_channels\n",
        "    )\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({\n",
        "            'expand_channels': self.expand_channels,\n",
        "            'output_channels': self.output_channels,\n",
        "            'strides': self.strides,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def call(self, x):\n",
        "    o = self.expand(x)\n",
        "    o = self.dw_conv(o)\n",
        "    o = self.pw_conv(o)\n",
        "    if self.strides == 1 and o.shape[-1] == self.output_channels:\n",
        "      return o + x\n",
        "    return o\n",
        "\n",
        "class FullyConnected(tf.keras.layers.Layer):\n",
        "  def __init__(self, hidden_units, dropout_rate):\n",
        "    super().__init__()\n",
        "    l = []\n",
        "    for units in hidden_units:\n",
        "      l.append(tf.keras.layers.Dense(units, activation=tf.nn.swish))\n",
        "      l.append(tf.keras.layers.Dropout(dropout_rate))\n",
        "    self.mlp = tf.keras.models.Sequential(l)\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    pass\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.mlp(x)\n",
        "\n",
        "class Transformer(tf.keras.layers.Layer):\n",
        "  def __init__(self, projection_dim, heads=2):\n",
        "    super().__init__()\n",
        "    self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=heads, key_dim=projection_dim, dropout=0.1)\n",
        "    self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    self.mlp = FullyConnected(\n",
        "        [input_shape[-1] * 2, input_shape[-1]], dropout_rate=0.1)\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    pass\n",
        "\n",
        "  def call(self, x):\n",
        "    x1 = self.norm1(x)\n",
        "    att = self.attention(x1, x1)\n",
        "    x2 = x + att\n",
        "    x3 = self.norm2(x2)\n",
        "    x3 = self.mlp(x3)\n",
        "    return x3 + x2\n",
        "\n",
        "class MobileVitBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, projection_dim, strides=1, patch_size=4):\n",
        "    super().__init__()\n",
        "    self.projection_dim = projection_dim\n",
        "    self.num_blocks = num_blocks\n",
        "    self.patch_size = patch_size\n",
        "    self.strides = strides\n",
        "    self.conv_local = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(projection_dim, 3, padding=\"same\", strides=strides, activation=tf.nn.swish),\n",
        "        tf.keras.layers.Conv2D(projection_dim, 1, padding=\"same\", strides=strides, activation=tf.nn.swish),\n",
        "                                           ])\n",
        "    self.transformers = tf.keras.models.Sequential([Transformer(projection_dim, heads=2) for i in range(num_blocks)])\n",
        "    self.conv_folded = tf.keras.layers.Conv2D(projection_dim, 1, padding=\"same\", strides=strides, activation=tf.nn.swish)\n",
        "    self.conv_local_global = tf.keras.layers.Conv2D(projection_dim, 3, padding=\"same\", strides=strides, activation=tf.nn.swish)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    num_patches = int((input_shape[1] * input_shape[2]) / self.patch_size)\n",
        "    self.unfold = tf.keras.layers.Reshape((self.patch_size, num_patches, self.projection_dim))\n",
        "    self.fold = tf.keras.layers.Reshape((input_shape[1], input_shape[2], self.projection_dim))\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({\n",
        "            'num_blocks': self.num_blocks,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'strides': self.strides,\n",
        "            'patch_size' : self.patch_size,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], input_shape[1], input_shape[2], self.projection_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "    local_features = self.conv_local(x)\n",
        "    patches = self.unfold(local_features)\n",
        "    global_features = self.transformers(patches)\n",
        "    folded_features = self.fold(global_features)\n",
        "    folded_features = self.conv_folded(folded_features)\n",
        "    local_global_features = tf.concat([x, folded_features], axis=-1)\n",
        "    local_global_features = self.conv_local_global(local_global_features)\n",
        "    return local_global_features\n",
        "\n",
        "def MobileViT(input_shape=None, include_top=True, classes=1000, expansion_ratio = 2.0):\n",
        "\n",
        "    img_input = tf.keras.layers.Input(shape=input_shape) # (None, 256, 256, 3)\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        16, 3, padding=\"same\", strides=(2, 2), activation=tf.nn.swish\n",
        "      )(img_input)  # (None, 128, 128, 16) \n",
        "    x = InvertedRes(16 * expansion_ratio, 16, strides=1)(x) # (None, 128, 128, 16)\n",
        "    x = InvertedRes(16 * expansion_ratio, 24, strides=2)(x) # (None, 64, 64, 24) \n",
        "    x = InvertedRes(24 * expansion_ratio, 24, strides=1)(x) # (None, 64, 64, 24)\n",
        "    x = InvertedRes(24 * expansion_ratio, 24, strides=1)(x) # (None, 64, 64, 24)\n",
        "    x = InvertedRes(24 * expansion_ratio, 48, strides=2)(x) # (None, 32, 32, 48)\n",
        "    x = MobileVitBlock(2, 64, strides=1)(x)                 # (None, 32, 32, 64)\n",
        "    x = InvertedRes(64 * expansion_ratio, 64, strides=2)(x) # (None, 16, 16, 64)\n",
        "    x = MobileVitBlock(4, 80, strides=1)(x)                 # (None, 16, 16, 80)\n",
        "    x = InvertedRes(80 * expansion_ratio, 80, strides=2)(x) # (None, 8, 8, 80)\n",
        "    x = MobileVitBlock(3, 96, strides=1)(x)                 # (None, 8, 8, 96)\n",
        "\n",
        "    if include_top:\n",
        "        x = tf.keras.layers.Conv2D(320, 1, padding=\"same\", strides=(1, 1), activation=tf.nn.swish)(x)\n",
        "        x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "        x = tf.keras.layers.Dense(classes, activation=\"sigmoid\")(x)\n",
        "        \n",
        "    # Create model.\n",
        "    model = tf.keras.models.Model(img_input, x)#, name='MobileViT')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIaOPLs__jq6",
        "outputId": "c805d999-8d02-4642-fac1-8de45390cbad"
      },
      "outputs": [],
      "source": [
        "# MobileViT model check\n",
        "\n",
        "#model = MobileViT((256, 256, 3), include_top=True, classes=1)\n",
        "#model.build((None, 256, 256, 3))\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MRVrzrV2BC"
      },
      "source": [
        "# Model (Temporal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9pcBc0eV1eY"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.Dense(dense_dim, activation=tf.nn.gelu), \n",
        "                tf.keras.layers.Dense(embed_dim),\n",
        "             ]\n",
        "        )\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'dense_dim': self.dense_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "        })\n",
        "        return config\n",
        "        \n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSkqvv2gWMI_"
      },
      "outputs": [],
      "source": [
        "def TransformerModel(embed_dim, dense_dim=512, num_heads=4, num_classes=4):\n",
        "  inputs = tf.keras.Input(shape=(512, embed_dim))\n",
        "  x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(inputs)\n",
        "  x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ljqw_h3SIYv",
        "outputId": "1d3f7b8c-4e9e-4424-bc34-c112acdd02ae"
      },
      "outputs": [],
      "source": [
        "# Transformer Encoder model check\n",
        "\n",
        "#temporal_model = TransformerModel(embed_dim=512)\n",
        "#temporal_model.build((None, 512, 512))\n",
        "#temporal_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeVazgqPV54N"
      },
      "source": [
        "# Model (3D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Movinet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0FYoZoIT3YX"
      },
      "outputs": [],
      "source": [
        "model_id = 'a3'\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(model_id=model_id)\n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "model.build([1, 1, 1, 1, 3])\n",
        "\n",
        "# Load pretrained weights\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a3_base.tar.gz -O movinet_a3_base.tar.gz -q\n",
        "!tar -xvf movinet_a3_base.tar.gz\n",
        "\n",
        "def build_classifier(hp, backbone, num_classes, freeze_backbone=False, many2one=True):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "\n",
        "  backbone = movinet.Movinet(model_id=model_id)\n",
        "  hp_dropout = hp.Float('dropout', 0, 0.2, step=0.1, default=0.2)\n",
        "  hp_activation = hp.Choice('activation_func', values=['swish', 'sigmoid', 'tanh'])\n",
        "  model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "  model.build([1, 1, 1, 1, 3])\n",
        "  if many2one:\n",
        "    model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  \n",
        "    #model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "  else:\n",
        "    model_encoder = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes * SEQ_LEN, \n",
        "      dropout_rate = hp_dropout)\n",
        "    \n",
        "    temporal_reshape = tf.keras.layers.Reshape(\n",
        "        (SEQ_LEN, num_classes)\n",
        "      )\n",
        "    model = tf.keras.Sequential([\n",
        "          model_encoder, \n",
        "          temporal_reshape, \n",
        "      ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c9oSYjWU5qY"
      },
      "source": [
        "# Model (hp search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WCWYCxObBDh"
      },
      "outputs": [],
      "source": [
        "def create_model_for_parameter_search(hp, encoder_name, temporal_model_name='lstm', many2one=True):\n",
        "\n",
        "  temporal_input = tf.keras.layers.Input(shape=(SEQ_LEN, IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS))\n",
        "  \n",
        "  # Encoder model\n",
        "  if encoder_name == 'mobilenetv2':    # MobileNetV2 model\n",
        "    \n",
        "    # model weights initialization: pretrained on Imagenet or random \n",
        "    hp_weights_init = hp.Choice('hp_mobilenet_weights_init', values=['imagenet', 'None'])\n",
        "    # alpha (expansion ratio)\n",
        "    hp_alpha = hp.Choice('hp_expansion_ration', values=['0.35, 0.50, 0.75, 1.0, 1.3, 1.4'])\n",
        "\n",
        "\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "      input_tensor=tf.keras.layers.Input(shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS)),\n",
        "      input_shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS),\n",
        "      include_top=False,\n",
        "      weights= hp_weights_init if hp_weights_init != 'None' else None \n",
        "    )\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "\n",
        "  elif encoder_name == 'vgg':   # VGG 19 model\n",
        "    base_model = tf.keras.applications.vgg19.VGG19(\n",
        "        input_shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "\n",
        "  elif encoder_name == 'efficient_b0':    # EfficientNet b0 model\n",
        "    base_model = tf.keras.applications.EfficientNetB0(\n",
        "        input_shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS),\n",
        "        include_top=False, \n",
        "        weights='imagenet'\n",
        "    )\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "\n",
        "  elif encoder_name == 'mobilevit':     # MobileViT model\n",
        "    hp_expansion_ratio = hp.Float('expansion_ratio', 1.0, 4.0, step=1., default=1.0)\n",
        "    \n",
        "    base_model = MobileViT(\n",
        "      input_shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS),\n",
        "      include_top=False,\n",
        "      expansion_ratio = hp_expansion_ratio\n",
        "    )\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "\n",
        "  elif encoder_name == 'vit':     # Visual Transformer base 16 model\n",
        "    # https://github.com/faustomorales/vit-keras/blob/28815edc5c24492612af726d1b2ca78295128d84/vit_keras/vit.py\n",
        "    base_model = vit_b16(\n",
        "      image_size=IMAGE_CROP_SIZE,\n",
        "      activation='sigmoid',\n",
        "      pretrained=True,\n",
        "      include_top=False,\n",
        "      pretrained_top=False,\n",
        "    )\n",
        "    x = base_model.layers[-2].output\n",
        "    x = SliceClassToken()(x) \n",
        "    patch_size = int(math.sqrt(x.shape[-2]))  \n",
        "    embedding_dim = x.shape[-1]\n",
        "    embedding = tf.keras.layers.Reshape((\n",
        "      patch_size, patch_size, \n",
        "      embedding_dim))(x)\n",
        "    base_model = tf.keras.Model(inputs=base_model.input, outputs=embedding)\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "  # number of layers to train\n",
        "  hp_num_trained_layers = hp.Int('num_trained_layers', min_value=1, max_value=len(base_model.layers), step=1)\n",
        "  fine_tune_at = FINE_TUNE_BUFFER\n",
        "  for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "  # Intermidiary layers to match shapes of encoder outputs and temporal model inputs\n",
        "  base_model = tf.keras.layers.TimeDistributed(\n",
        "      base_model, \n",
        "      input_shape=(SEQ_LEN, IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS))\n",
        "\n",
        "  global_avg_pool = tf.keras.layers.TimeDistributed(\n",
        "      tf.keras.layers.GlobalAveragePooling2D(),\n",
        "      input_shape=(8, 8, 96))\n",
        "  flatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
        "\n",
        "  hp_dropout = hp.Float('dropout', 0, 0.5, step=0.1, default=0.5)\n",
        "  dropout = tf.keras.layers.Dropout(hp_dropout)\n",
        "\n",
        "\n",
        "  # temporal model\n",
        "  if temporal_model_name == 'lstm':        # LSTM temporal model\n",
        "    hp_dropout_lstm = hp.Float('dropout_lstm', 0, 0.5, step=0.1, default=0.5)\n",
        "    hp_dropout_lstm_recurrent = hp.Float('dropout_lstm_recurrent', 0, 0.5, step=0.1, default=0.5)\n",
        "    hp_use_bias = hp.Choice('bias', values=[True, False])\n",
        "    # number of LSTM units to use\n",
        "    hp_units = hp.Int('lstm_units', 2, 24, step=2, default=16)\n",
        "    temporal_layer = tf.keras.layers.LSTM(\n",
        "        hp_units, return_sequences= not many2one, dropout=hp_dropout_lstm, use_bias=hp_use_bias, recurrent_dropout=hp_dropout_lstm_recurrent)\n",
        "\n",
        "  elif temporal_model_name == 'transformer':      # Transformer Encoder temporal model\n",
        "    hp_dense_dim = hp.Int('transformer_dense_dim', min_value=64, max_value=1024, step=64)\n",
        "    hp_num_heads = hp.Int('transformer_num_heads', min_value=2, max_value=8, step=1)\n",
        "    temporal_layer = TransformerEncoder(\n",
        "        encoder_output_shape[-1], dense_dim=hp_dense_dim, num_heads=hp_num_heads, \n",
        "        name=\"transformer_layer\"\n",
        "    )\n",
        "    if many2one == True:      # many 2 one classification head of Transformer model\n",
        "      temporal_reshape = tf.keras.layers.Reshape(\n",
        "        (SEQ_LEN * encoder_output_shape[-1], )\n",
        "      )\n",
        "      # number of dense units to select\n",
        "      hp_dense_units = hp.Int('transformer_dense_units', min_value=64, max_value=512, steps=64)\n",
        "      linear = tf.keras.layers.Dense(hp_dense_units, activation='relu')\n",
        "      temporal_layer = tf.keras.Sequential([\n",
        "          temporal_layer, \n",
        "          temporal_reshape, \n",
        "          linear\n",
        "      ])\n",
        "\n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "  # output layer\n",
        "  dense = tf.keras.layers.Dense(4, activation='softmax')\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      temporal_input,\n",
        "      base_model,\n",
        "      global_avg_pool,\n",
        "      flatten,\n",
        "      dropout,\n",
        "      temporal_layer,\n",
        "      dense\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEDIUim9cvNQ"
      },
      "outputs": [],
      "source": [
        "def build_hp_search_model(hp):\n",
        "\n",
        "  # selecting learning rate\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
        "\n",
        "  # selecting learning rate scheduler\n",
        "  hp_scheduler = hp.Choice('lr_scheduler', values=['cosine_decay', 'cosine_decay_restart'])\n",
        "  if hp_scheduler == 'cosine_decay':\n",
        "    lr = tf.keras.experimental.CosineDecay(hp_learning_rate, STEPS_PER_EPOCH * EPOCHS)\n",
        "  elif hp_scheduler == 'cosine_decay_restart':\n",
        "    lr = tf.keras.experimental.CosineDecayRestarts(hp_learning_rate, STEPS_PER_EPOCH * EPOCHS) \n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "  with strategy.scope():\n",
        "    # creating a model\n",
        "    if ENCODER_NAME == 'movinet':\n",
        "      model = build_classifier(hp, backbone, num_classes=4, freeze_backbone=False, many2one=MANY2ONE)\n",
        "    else:\n",
        "      model = create_model_for_parameter_search(hp, ENCODER_NAME, TEMPORAL_MODEL, MANY2ONE)\n",
        "\n",
        "    # selecting optimizer \n",
        "    hp_optimizer = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop', 'adagrad'])\n",
        "    if hp_optimizer == 'adam':\n",
        "      optimizer = tf.keras.optimizers.Adam(lr) # adam optimizer\n",
        "    elif hp_optimizer == 'sgd':\n",
        "      optimizer = tf.keras.optimizers.SGD(lr)\n",
        "    elif hp_optimizer == 'rmsprop':\n",
        "      optimizer = tf.keras.optimizers.RMSprop(lr)\n",
        "    elif hp_optimizer == 'adagrad':\n",
        "      optimizer = tf.keras.optimizers.Adagrad(lr)\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "\n",
        "    # selecting loss function\n",
        "    hp_loss = hp.Choice('loss_function', values=['ce', 'focal'])\n",
        "    if hp_loss == 'ce':\n",
        "      loss_func = 'sparse_categorical_crossentropy'\n",
        "    elif hp_loss == 'focal':\n",
        "      loss_func = SparseCategoricalFocalLoss(gamma=2)\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "      \n",
        "    model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss = loss_func,\n",
        "      # predict True positives/total images\n",
        "      metrics=['accuracy'],\n",
        "      # NEW on TPU in TensorFlow 24: sending multiple batches to the TPU at once saves communications\n",
        "      # overheads and allows the XLA compiler to unroll the loop on TPU and optimize hardware utilization.\n",
        "      steps_per_execution=16\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXrAPBH0bQHQ"
      },
      "outputs": [],
      "source": [
        "# creating tuning class\n",
        "tuner = kt.Hyperband(create_model_for_parameter_search,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=5,\n",
        "                     factor=3,\n",
        "                     directory='drive/MyDrive/my_dir/',\n",
        "                     project_name=f'hyperparameter_search_{ENCODER_NAME}_{TEMPORAL_MODEL}_Many2one:{MANY2ONE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj_enFmvbgO1"
      },
      "outputs": [],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def new_on_epoch_end(self, epoch, logs=None):\n",
        "    if not self.objective.has_value(logs):\n",
        "            # Save on every epoch if metric value is not in the logs. Either no\n",
        "            # objective is specified, or objective is computed and returned\n",
        "            # after `fit()`.\n",
        "\n",
        "            #***** the following are the lines I added ******************************************\n",
        "            # Save model in Tensorflow's \"SavedModel\" format\n",
        "            \n",
        "        save_locally = tf.saved_model.SaveOptions(experimental_io_device = '/job:localhost')\n",
        "            \n",
        "            # I then added ', options = save_locally' to the line below.\n",
        "            #************************************************************************************\n",
        "        \n",
        "        self.model.save_weights(self.filepath, options = save_locally)\n",
        "        return\n",
        "    current_value = self.objective.get_value(logs)\n",
        "    if self.objective.better_than(current_value, self.best_value):\n",
        "        self.best_value = current_value\n",
        "            \n",
        "            #***** the following are the lines I added ******************************************\n",
        "            # Save model in Tensorflow's \"SavedModel\" format\n",
        "            \n",
        "        save_locally = tf.saved_model.SaveOptions(experimental_io_device = '/job:localhost')\n",
        "            \n",
        "            # I then added ', options = save_locally' to the line below.\n",
        "            #************************************************************************************\n",
        "            \n",
        "        self.model.save_weights(self.filepath, options = save_locally)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amc1q16lbg-3"
      },
      "outputs": [],
      "source": [
        "from mock import patch\n",
        "\n",
        "with patch('keras_tuner.engine.tuner_utils.SaveBestEpoch.on_epoch_end', new_on_epoch_end):\n",
        "  tuner.search(\n",
        "    get_train_dataset(), \n",
        "    steps_per_epoch=STEPS_PER_EPOCH, \n",
        "    validation_data=get_validation_dataset(repeated=True), \n",
        "    validation_steps=VALIDATION_STEPS,\n",
        "    use_multiprocessing=True,\n",
        "    epochs=5, \n",
        "    callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(best_hps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars(best_hps)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "vid_cl_tf_tpu_pipeline.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
