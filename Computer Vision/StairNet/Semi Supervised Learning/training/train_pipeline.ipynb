{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R94pjX_-bQHa"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "  tpu_worker = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466') \n",
        "  print(tf.profiler.experimental.client.monitor(tpu_worker,1))\n",
        "except ValueError:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  #raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asp1UpAHW4Ol",
        "outputId": "88cbc161-0d1c-471f-a650-d80a94c729fd"
      },
      "outputs": [],
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip install opencv-python==4.5.5.64\n",
        "!pip install opencv-contrib-python\n",
        "!pip install tensorflow_addons\n",
        "!pip install vit-keras\n",
        "!pip3 install tf-models-official\n",
        "!pip install -q -U keras-tuner\n",
        "!pip install focal-loss\n",
        "\n",
        "!pip uninstall tensorflow --yes\n",
        "!pip install tensorflow==2.8.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Cu8VzAbdN2",
        "outputId": "144b5779-19fc-44d0-d94a-4b0464c11440"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'stiarnet-v2'\n",
        "\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xSI8sKr4_qj",
        "outputId": "6fc91d66-f1ff-463d-8e3b-41fcb8cb41c4"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -al gs://"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j-Ut9Y_UbaM",
        "outputId": "7780b4c1-824e-4b66-d529-7d37c6fcaf93"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4ofSOy3UsP2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import ast\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from tqdm import trange\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "import typing_extensions as tx\n",
        "import matplotlib.pyplot as plt\n",
        "from vit_keras import vit, utils\n",
        "from sklearn.metrics import accuracy_score\n",
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "\n",
        "from official.vision.configs import video_classification\n",
        "from official.projects.movinet.configs import movinet as movinet_configs\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_layers\n",
        "from official.projects.movinet.modeling import movinet_model\n",
        "from official.projects.movinet.tools import export_saved_model\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "bBpAGNLfs_pA",
        "outputId": "0107cab6-9623-4d45-ddaa-16db4154d22a"
      },
      "outputs": [],
      "source": [
        "tf.__version__, np.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fuE4SB0ZzrQ"
      },
      "outputs": [],
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CigmXccqUt7I",
        "outputId": "21c1be24-bb95-47b4-edc0-6f7b8ebfa6af"
      },
      "outputs": [],
      "source": [
        "# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU/GPU/multi-GPU/cluster-GPU detection code\n",
        "\n",
        "try: # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: # detect GPUs\n",
        "    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n",
        "\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsFknIAYynfs"
      },
      "source": [
        "# FLAGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNh-oEmHmorI",
        "outputId": "89872a62-d6f8-49ca-d174-f8fc922fc405"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "CLASS_MAP = {'IS': 0 , 'ISLG': 1, 'LG': 2, 'LGIS': 3}       \n",
        "\n",
        "GCS_PATH = 'gs://stairnet-v2/StairNet_Video_splits/'\n",
        "print(GCS_PATH)\n",
        "\n",
        "\n",
        "TRAIN_FILENAMES = tf.io.gfile.glob(GCS_PATH + 'train_*.tfrecord')\n",
        "print(TRAIN_FILENAMES)\n",
        "VAL_FILENAMES = tf.io.gfile.glob(GCS_PATH + 'val_*.tfrecord')\n",
        "print(VAL_FILENAMES)\n",
        "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + 'test_*.tfrecord') # predictions on this dataset should be submitted for the competition\n",
        "print(TEST_FILENAMES)\n",
        "\n",
        "\n",
        "# set the number of epochs for the run                        \n",
        "EPOCHS = 10\n",
        "\n",
        "# set the initial learning rate\n",
        "BASE_LR = 0.00001\n",
        "\n",
        "# 8 TPU cores, so 16 will be 128\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "BATCH_SIZE_VAL = 32 * strategy.num_replicas_in_sync\n",
        "\n",
        "#To study the effect of transfer training:\n",
        "FINE_TUNE_BUFFER = -1\n",
        "\n",
        "# Buffer size of the dataset\n",
        "BUFFER_SIZE = 100\n",
        "\n",
        "#Change Droprate \n",
        "DROPOUT_VALUE = 0.2\n",
        "\n",
        "# For hyperparameter optimization set to 144 \n",
        "# For final run set to 'None'\n",
        "SEED_NUMBER = 144 \n",
        "\n",
        "SEQ_LEN = 5\n",
        "INIT_IMAGE_SIZE = 256\n",
        "IMAGE_CROP_SIZE = 256\n",
        "NUM_CHANNELS = 3\n",
        "\n",
        "ENCODER_NAME = 'efficient_b0' # ['movinet', 'mobilevit', 'vit', 'vgg', 'mobilenetv2', 'efficient_b0']\n",
        "TEMPORAL_MODEL = 'lstm' # ['transformer', 'lstm']\n",
        "MANY2ONE = True\n",
        "\n",
        "OPTIMIZER = 'adam'\n",
        "\n",
        "EVALUATE_INFERENCE_SPEED = False\n",
        "CHECK_UPSAMPLE_DISTRBUTION = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "caJkbwcd1wqe",
        "outputId": "9750b930-5bed-40c2-a7d5-8eed511333e6"
      },
      "outputs": [],
      "source": [
        "len(TRAIN_FILENAMES), len(VAL_FILENAMES), len(TEST_FILENAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6xVBoH3yxy8"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_c7TBECrzv7"
      },
      "outputs": [],
      "source": [
        "def decode_image_seq(image_data):\n",
        "    ''' reading sequence from bytes array '''\n",
        "    image = tf.io.decode_raw(image_data, 'float64')\n",
        "    image = tf.reshape(image, [SEQ_LEN, INIT_IMAGE_SIZE, INIT_IMAGE_SIZE, NUM_CHANNELS]) # explicit size needed for TPU\n",
        "    image = tf.cast(image, tf.float32) / 255.\n",
        "    image = tf.image.random_crop(value=image, size=(SEQ_LEN, IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS))\n",
        "    return image\n",
        "\n",
        "def convert_str_label(tf_id):\n",
        "    ''' casting bytes to integer label '''\n",
        "    _id = tf_id.numpy().decode('utf-8')\n",
        "    _id = ast.literal_eval(_id)\n",
        "    labels = [CLASS_MAP[el] for el in _id][-1]\n",
        "    return tf.cast(labels, tf.int32)\n",
        "\n",
        "def read_labeled_tfrecord(example):\n",
        "    ''' reading sample from tfrecord '''\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    image_seq = decode_image_seq(example['image'])\n",
        "    label = tf.io.decode_raw(example['label'], 'int32')[-2]\n",
        "    return image_seq, label # returns a dataset of image(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AylPvGAZ2PC"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filenames, ordered=False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
        "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEIC1I5EU2Uv"
      },
      "outputs": [],
      "source": [
        "def get_validation_dataset(repeated=False, ordered=False, distributed=True):\n",
        "    dataset = load_dataset(VAL_FILENAMES)\n",
        "    if repeated:\n",
        "      dataset = dataset.repeat(EPOCHS)\n",
        "      # for hyperparameter testing using random seed to shuffle the data the same to elimite this variable from results\n",
        "      dataset = dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED_NUMBER, reshuffle_each_iteration=None)\n",
        "    dataset = dataset.batch(BATCH_SIZE_VAL, drop_remainder=repeated)\n",
        "    #dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    if distributed:\n",
        "        dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "    return dataset\n",
        "\n",
        "def get_train_dataset():\n",
        "    dataset = load_dataset(TRAIN_FILENAMES)\n",
        "    dataset = dataset.repeat()\n",
        "    # for hyperparameter testing using random seed to shuffle the data the same to elimite this variable from results\n",
        "    dataset = dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED_NUMBER, reshuffle_each_iteration=None)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    #dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "    return dataset\n",
        "\n",
        "def get_test_dataset():\n",
        "    dataset = load_dataset(TEST_FILENAMES)\n",
        "    dataset = dataset.batch(BATCH_SIZE_VAL)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "debZaxD8xfmf"
      },
      "outputs": [],
      "source": [
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FSrVI2lxdrf"
      },
      "outputs": [],
      "source": [
        "NUM_TRAINING_IMAGES = 426177 \n",
        "NUM_VALIDATION_IMAGES = 32487 \n",
        "NUM_TEST_IMAGES = 56729\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "VALIDATION_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "TEST_STEPS = NUM_TEST_IMAGES // BATCH_SIZE            # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "print('Dataset: {} training images, {} validation images, {} test images'.format(\n",
        "    NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))\n",
        "print('Dataset: {} training steps, {} validation steps, {} test steps'.format(\n",
        "    STEPS_PER_EPOCH, VALIDATION_STEPS, TEST_STEPS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqDtCh8q00v6"
      },
      "outputs": [],
      "source": [
        "# Get labels and their countings\n",
        "from collections import Counter\n",
        "\n",
        "def get_training_dataset_raw():\n",
        "    dataset = load_dataset(TRAIN_FILENAMES, ordered=False)\n",
        "    return dataset\n",
        "\n",
        "raw_training_dataset = get_training_dataset_raw() # default dataset \n",
        "\n",
        "label_counter = Counter()\n",
        "for images, labels in raw_training_dataset:\n",
        "    label_counter.update([labels.numpy()])\n",
        "\n",
        "del raw_training_dataset    \n",
        "    \n",
        "label_counting_sorted = label_counter.most_common()\n",
        "\n",
        "NUM_TRAINING_IMAGES = sum([x[1] for x in label_counting_sorted])\n",
        "print(\"number of examples in the original training dataset: {}\".format(NUM_TRAINING_IMAGES))\n",
        "\n",
        "print(\"labels in the original training dataset, sorted by occurrence\")\n",
        "print(label_counting_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc-Wpc7R0SUG"
      },
      "outputs": [],
      "source": [
        "# We want each class occur at least (approximately) `TARGET_MIN_COUNTING` times\n",
        "\n",
        "TARGET_MIN_COUNTING = 300000\n",
        "\n",
        "def get_num_of_repetition_for_class(class_id):\n",
        "    counting = label_counter[class_id]\n",
        "    if counting >= TARGET_MIN_COUNTING:\n",
        "        return 1.0\n",
        "    num_to_repeat = TARGET_MIN_COUNTING / counting\n",
        "    return num_to_repeat\n",
        "\n",
        "numbers_of_repetition_for_classes = {class_id: get_num_of_repetition_for_class(class_id) for class_id in range(4)}\n",
        "\n",
        "print(\"number of repetitions for each class (if > 1)\")\n",
        "{k: v for k, v in sorted(numbers_of_repetition_for_classes.items(), key=lambda item: item[1], reverse=True) if v > 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zLYAlPpOXgl"
      },
      "outputs": [],
      "source": [
        "keys_tensor = tf.constant([k for k in numbers_of_repetition_for_classes])\n",
        "vals_tensor = tf.constant([numbers_of_repetition_for_classes[k] for k in numbers_of_repetition_for_classes])\n",
        "table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), -1)\n",
        "\n",
        "def get_num_of_repetition_for_example(train_sample):\n",
        "  ''' counting number of samples that share the same label '''\n",
        "  _, label  = train_sample\n",
        "  num_to_repeat = table.lookup(label)\n",
        "  num_to_repeat_integral = tf.cast(int(num_to_repeat), tf.float32)\n",
        "  residue = num_to_repeat - num_to_repeat_integral\n",
        "  num_to_repeat = num_to_repeat_integral + tf.cast(tf.random.uniform(shape=()) <= residue, tf.float32)\n",
        "  return tf.cast(num_to_repeat, tf.int64)\n",
        "\n",
        "def get_train_dataset_with_oversample(oversample=False):\n",
        "  ''' costructing new dataset with class oversampling '''\n",
        "  dataset = load_dataset(TRAIN_FILENAMES)\n",
        "\n",
        "  if oversample:\n",
        "    dataset = dataset.flat_map(\n",
        "        lambda sequence, label: tf.data.Dataset.from_tensors((sequence, label)).repeat(\n",
        "            get_num_of_repetition_for_example((sequence, label))\n",
        "        )\n",
        "    )\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.shuffle(20000)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "  dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGZItKdGU2my"
      },
      "source": [
        "# Model (Encoders)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNxy7ycJqtRE"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://github.com/faustomorales/vit-keras/releases/download/dl\"\n",
        "WEIGHTS = {\"imagenet21k\": 21_843, \"imagenet21k+imagenet2012\": 1_000}\n",
        "\n",
        "ConfigDict = tx.TypedDict(\n",
        "    \"ConfigDict\",\n",
        "    {\n",
        "        \"dropout\": float,\n",
        "        \"mlp_dim\": int,\n",
        "        \"num_heads\": int,\n",
        "        \"num_layers\": int,\n",
        "        \"hidden_size\": int,\n",
        "    },\n",
        ")\n",
        "\n",
        "CONFIG_B: ConfigDict = {\n",
        "    \"dropout\": 0.1,\n",
        "    \"mlp_dim\": 3072,\n",
        "    \"num_heads\": 12,\n",
        "    \"num_layers\": 12,\n",
        "    \"hidden_size\": 768,\n",
        "}\n",
        "\n",
        "class AddPositionEmbs(tf.keras.layers.Layer):\n",
        "    \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert (\n",
        "            len(input_shape) == 3\n",
        "        ), f\"Number of dimensions should be 3, got {len(input_shape)}\"\n",
        "        self.pe = tf.Variable(\n",
        "            name=\"pos_embedding\",\n",
        "            initial_value=tf.random_normal_initializer(stddev=0.06)(\n",
        "                shape=(1, input_shape[1], input_shape[2])\n",
        "            ),\n",
        "            dtype=\"float32\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[2])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + tf.cast(self.pe, dtype=inputs.dtype)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *args, num_heads, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        hidden_size = input_shape[-1]\n",
        "        num_heads = self.num_heads\n",
        "        if hidden_size % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {hidden_size} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.hidden_size = hidden_size\n",
        "        self.projection_dim = hidden_size // num_heads\n",
        "        self.query_dense = tf.keras.layers.Dense(hidden_size, name=\"query\")\n",
        "        self.key_dense = tf.keras.layers.Dense(hidden_size, name=\"key\")\n",
        "        self.value_dense = tf.keras.layers.Dense(hidden_size, name=\"value\")\n",
        "        self.combine_heads = tf.keras.layers.Dense(hidden_size, name=\"out\")\n",
        "\n",
        "    # pylint: disable=no-self-use\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], score.dtype)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.hidden_size))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output, weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_heads\": self.num_heads})\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"Implements a Transformer block.\"\"\"\n",
        "\n",
        "    def __init__(self, *args, num_heads, mlp_dim, dropout, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.att = MultiHeadSelfAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            name=\"MultiHeadDotProductAttention_1\",\n",
        "        )\n",
        "        self.mlpblock = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.Dense(\n",
        "                    self.mlp_dim,\n",
        "                    activation=\"linear\",\n",
        "                    name=f\"{self.name}/Dense_0\",\n",
        "                ),\n",
        "                tf.keras.layers.Lambda(\n",
        "                    lambda x: tf.keras.activations.gelu(x, approximate=False)\n",
        "                )\n",
        "                if hasattr(tf.keras.activations, \"gelu\")\n",
        "                else tf.keras.layers.Lambda(\n",
        "                    lambda x: tf.activations.gelu(x, approximate=False)\n",
        "                ),\n",
        "                tf.keras.layers.Dropout(self.dropout),\n",
        "                tf.keras.layers.Dense(input_shape[-1], name=f\"{self.name}/Dense_1\"),\n",
        "                tf.keras.layers.Dropout(self.dropout),\n",
        "            ],\n",
        "            name=\"MlpBlock_3\",\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6, name=\"LayerNorm_0\"\n",
        "        )\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6, name=\"LayerNorm_2\"\n",
        "        )\n",
        "        self.dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.layernorm1(inputs)\n",
        "        x, weights = self.att(x)\n",
        "        x = self.dropout_layer(x, training=training)\n",
        "        x = x + inputs\n",
        "        y = self.layernorm2(x)\n",
        "        y = self.mlpblock(y)\n",
        "        return x + y, weights\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[2])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"mlp_dim\": self.mlp_dim,\n",
        "                \"dropout\": self.dropout,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "class ClassToken(tf.keras.layers.Layer):\n",
        "    \"\"\"Append a class token to an input layer.\"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        cls_init = tf.zeros_initializer()\n",
        "        self.hidden_size = input_shape[-1]\n",
        "        self.cls = tf.Variable(\n",
        "            name=\"cls\",\n",
        "            initial_value=cls_init(shape=(1, 1, self.hidden_size), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1] + 1, input_shape[2])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        cls_broadcasted = tf.cast(\n",
        "            tf.broadcast_to(self.cls, [batch_size, 1, self.hidden_size]),\n",
        "            dtype=inputs.dtype,\n",
        "        )\n",
        "        return tf.concat([cls_broadcasted, inputs], 1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "class SliceClassToken(tf.keras.layers.Layer):\n",
        "    \"\"\"Append a class token to an input layer.\"\"\"\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1] - 1, input_shape[2])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs[:, :-1, :]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "def interpret_image_size(image_size_arg):\n",
        "    \"\"\"Process the image_size argument whether a tuple or int.\"\"\"\n",
        "    if isinstance(image_size_arg, int):\n",
        "        return (image_size_arg, image_size_arg)\n",
        "    if (\n",
        "        isinstance(image_size_arg, tuple)\n",
        "        and len(image_size_arg) == 2\n",
        "        and all(map(lambda v: isinstance(v, int), image_size_arg))\n",
        "    ):\n",
        "        return image_size_arg\n",
        "    raise ValueError(\n",
        "        f\"The image_size argument must be a tuple of 2 integers or a single integer. Received: {image_size_arg}\"\n",
        "    )\n",
        "\n",
        "def build_model(\n",
        "    image_size,\n",
        "    patch_size: int,\n",
        "    num_layers: int,\n",
        "    hidden_size: int,\n",
        "    num_heads: int,\n",
        "    name: str,\n",
        "    mlp_dim: int,\n",
        "    classes: int,\n",
        "    dropout=0.1,\n",
        "    activation=\"linear\",\n",
        "    include_top=True,\n",
        "    representation_size=None,\n",
        "):\n",
        "    \"\"\"Build a ViT model.\n",
        "    Args:\n",
        "        image_size: The size of input images.\n",
        "        patch_size: The size of each patch (must fit evenly in image_size)\n",
        "        classes: optional number of classes to classify images\n",
        "            into, only to be specified if `include_top` is True, and\n",
        "            if no `weights` argument is specified.\n",
        "        num_layers: The number of transformer layers to use.\n",
        "        hidden_size: The number of filters to use\n",
        "        num_heads: The number of transformer heads\n",
        "        mlp_dim: The number of dimensions for the MLP output in the transformers.\n",
        "        dropout_rate: fraction of the units to drop for dense layers.\n",
        "        activation: The activation to use for the final layer.\n",
        "        include_top: Whether to include the final classification layer. If not,\n",
        "            the output will have dimensions (batch_size, hidden_size).\n",
        "        representation_size: The size of the representation prior to the\n",
        "            classification layer. If None, no Dense layer is inserted.\n",
        "    \"\"\"\n",
        "    image_size_tuple = interpret_image_size(image_size)\n",
        "    assert (image_size_tuple[0] % patch_size == 0) and (\n",
        "        image_size_tuple[1] % patch_size == 0\n",
        "    ), \"image_size must be a multiple of patch_size\"\n",
        "    x = tf.keras.layers.Input(shape=(image_size_tuple[0], image_size_tuple[1], 3))\n",
        "    y = tf.keras.layers.Conv2D(\n",
        "        filters=hidden_size,\n",
        "        kernel_size=patch_size,\n",
        "        strides=patch_size,\n",
        "        padding=\"valid\",\n",
        "        name=\"embedding\",\n",
        "    )(x)\n",
        "    y = tf.keras.layers.Reshape((y.shape[1] * y.shape[2], hidden_size))(y)\n",
        "    y = ClassToken(name=\"class_token\")(y)\n",
        "    y = AddPositionEmbs(name=\"Transformer/posembed_input\")(y)\n",
        "    for n in range(num_layers):\n",
        "        y, _ = TransformerBlock(\n",
        "            num_heads=num_heads,\n",
        "            mlp_dim=mlp_dim,\n",
        "            dropout=dropout,\n",
        "            name=f\"Transformer/encoderblock_{n}\",\n",
        "        )(y)\n",
        "    y = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"Transformer/encoder_norm\"\n",
        "    )(y)\n",
        "    y = tf.keras.layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(y)\n",
        "    if representation_size is not None:\n",
        "        y = tf.keras.layers.Dense(\n",
        "            representation_size, name=\"pre_logits\", activation=\"tanh\"\n",
        "        )(y)\n",
        "    if include_top:\n",
        "        y = tf.keras.layers.Dense(classes, name=\"head\", activation=activation)(y)\n",
        "    return tf.keras.models.Model(inputs=x, outputs=y, name=name)\n",
        "\n",
        "\n",
        "def validate_pretrained_top(\n",
        "    include_top: bool, pretrained: bool, classes: int, weights: str\n",
        "):\n",
        "    \"\"\"Validate that the pretrained weight configuration makes sense.\"\"\"\n",
        "    assert weights in WEIGHTS, f\"Unexpected weights: {weights}.\"\n",
        "    expected_classes = WEIGHTS[weights]\n",
        "    assert include_top, \"Can only use pretrained_top with include_top.\"\n",
        "    assert pretrained, \"Can only use pretrained_top with pretrained.\"\n",
        "    return expected_classes\n",
        "\n",
        "\n",
        "def load_pretrained(\n",
        "    size: str,\n",
        "    weights: str,\n",
        "    pretrained_top: bool,\n",
        "    model: tf.keras.models.Model,\n",
        "    image_size,\n",
        "    patch_size: int,\n",
        "):\n",
        "    \"\"\"Load model weights for a known configuration.\"\"\"\n",
        "    image_size_tuple = interpret_image_size(image_size)\n",
        "    fname = f\"ViT-{size}_{weights}.npz\"\n",
        "    origin = f\"{BASE_URL}/{fname}\"\n",
        "    local_filepath = tf.keras.utils.get_file(fname, origin, cache_subdir=\"weights\")\n",
        "    utils.load_weights_numpy(\n",
        "        model=model,\n",
        "        params_path=local_filepath,\n",
        "        pretrained_top=pretrained_top,\n",
        "        num_x_patches=image_size_tuple[1] // patch_size,\n",
        "        num_y_patches=image_size_tuple[0] // patch_size,\n",
        "    )\n",
        "\n",
        "\n",
        "def vit_b16(\n",
        "    image_size = (224, 224),\n",
        "    classes=1000,\n",
        "    activation=\"linear\",\n",
        "    include_top=True,\n",
        "    pretrained=True,\n",
        "    pretrained_top=True,\n",
        "    weights=\"imagenet21k+imagenet2012\",\n",
        "):\n",
        "    \"\"\"Build ViT-B16. All arguments passed to build_model.\"\"\"\n",
        "    if pretrained_top:\n",
        "        classes = validate_pretrained_top(\n",
        "            include_top=include_top,\n",
        "            pretrained=pretrained,\n",
        "            classes=classes,\n",
        "            weights=weights,\n",
        "        )\n",
        "    model = build_model(\n",
        "        **CONFIG_B,\n",
        "        name=\"vit-b16\",\n",
        "        patch_size=16,\n",
        "        image_size=image_size,\n",
        "        classes=classes,\n",
        "        activation=activation,\n",
        "        include_top=include_top,\n",
        "        representation_size=768 if weights == \"imagenet21k\" else None,\n",
        "    )\n",
        "\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            size=\"B_16\",\n",
        "            weights=weights,\n",
        "            model=model,\n",
        "            pretrained_top=pretrained_top,\n",
        "            image_size=image_size,\n",
        "            patch_size=16,\n",
        "        )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKSD1ELaXk7o"
      },
      "outputs": [],
      "source": [
        "# Visual Transfromer model check\n",
        "\n",
        "#image_size = 224\n",
        "#base_model = vit_b16(\n",
        "#    image_size=image_size,\n",
        "#    activation='sigmoid',\n",
        "#    pretrained=True,\n",
        "#    include_top=False,\n",
        "#    pretrained_top=False,\n",
        "#)\n",
        "#x = base_model.layers[-2].output\n",
        "#x = SliceClassToken()(x) \n",
        "#patch_size = int(math.sqrt(x.shape[-2]))\n",
        "#embedding_dim = x.shape[-1]\n",
        "#embedding = tf.keras.layers.Reshape((\n",
        "#    patch_size, patch_size, \n",
        "#    embedding_dim))(x)\n",
        "#base_model = tf.keras.Model(inputs=base_model.input, outputs=embedding)\n",
        "#base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MobileViT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR4vbRIBLAou"
      },
      "outputs": [],
      "source": [
        "class InvertedRes(tf.keras.layers.Layer):\n",
        "  \"\"\"Inverted Residual Block\"\"\"\n",
        "  \n",
        "  def __init__(self, expand_channels, output_channels, strides=1):\n",
        "    super().__init__()\n",
        "    self.output_channels = output_channels\n",
        "    self.strides = strides\n",
        "    self.expand_channels = expand_channels\n",
        "    self.expand = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(expand_channels, 1, padding=\"same\", use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('swish')\n",
        "                                        ], name=\"expand\")\n",
        "    self.dw_conv = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.DepthwiseConv2D(3, strides=strides, padding=\"same\", use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('swish')\n",
        "                                        ], name=\"depthwise\")\n",
        "    self.pw_conv = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "                                        ], name='pointwise')\n",
        "  \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (\n",
        "      input_shape[0], \n",
        "      input_shape[1] // self.strides, \n",
        "      input_shape[2]  // self.strides, \n",
        "      self.output_channels\n",
        "    )\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({\n",
        "            'expand_channels': self.expand_channels,\n",
        "            'output_channels': self.output_channels,\n",
        "            'strides': self.strides,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def call(self, x):\n",
        "    o = self.expand(x)\n",
        "    o = self.dw_conv(o)\n",
        "    o = self.pw_conv(o)\n",
        "    if self.strides == 1 and o.shape[-1] == self.output_channels:\n",
        "      return o + x\n",
        "    return o\n",
        "\n",
        "class FullyConnected(tf.keras.layers.Layer):\n",
        "  \"\"\"Fully Connected Block\"\"\"\n",
        "\n",
        "  def __init__(self, hidden_units, dropout_rate):\n",
        "    super().__init__()\n",
        "    l = []\n",
        "    for units in hidden_units:\n",
        "      l.append(tf.keras.layers.Dense(units, activation=tf.nn.swish))\n",
        "      l.append(tf.keras.layers.Dropout(dropout_rate))\n",
        "    self.mlp = tf.keras.models.Sequential(l)\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    pass\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.mlp(x)\n",
        "\n",
        "class Transformer(tf.keras.layers.Layer):\n",
        "  \"\"\"Transformer Block\"\"\"\n",
        "  def __init__(self, projection_dim, heads=2):\n",
        "    super().__init__()\n",
        "    self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=heads, key_dim=projection_dim, dropout=0.1)\n",
        "    self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    self.mlp = FullyConnected(\n",
        "        [input_shape[-1] * 2, input_shape[-1]], dropout_rate=0.1)\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    pass\n",
        "\n",
        "  def call(self, x):\n",
        "    x1 = self.norm1(x)\n",
        "    att = self.attention(x1, x1)\n",
        "    x2 = x + att\n",
        "    x3 = self.norm2(x2)\n",
        "    x3 = self.mlp(x3)\n",
        "    return x3 + x2\n",
        "\n",
        "class MobileVitBlock(tf.keras.layers.Layer):\n",
        "  \"\"\"MobileViT Block\"\"\"\n",
        "\n",
        "  def __init__(self, num_blocks, projection_dim, strides=1, patch_size=4):\n",
        "    super().__init__()\n",
        "    self.projection_dim = projection_dim\n",
        "    self.num_blocks = num_blocks\n",
        "    self.patch_size = patch_size\n",
        "    self.strides = strides\n",
        "    self.conv_local = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(projection_dim, 3, padding=\"same\", strides=strides, activation=tf.nn.swish),\n",
        "        tf.keras.layers.Conv2D(projection_dim, 1, padding=\"same\", strides=strides, activation=tf.nn.swish),\n",
        "                                           ])\n",
        "    self.transformers = tf.keras.models.Sequential([Transformer(projection_dim, heads=2) for i in range(num_blocks)])\n",
        "    self.conv_folded = tf.keras.layers.Conv2D(projection_dim, 1, padding=\"same\", strides=strides, activation=tf.nn.swish)\n",
        "    self.conv_local_global = tf.keras.layers.Conv2D(projection_dim, 3, padding=\"same\", strides=strides, activation=tf.nn.swish)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    num_patches = int((input_shape[1] * input_shape[2]) / self.patch_size)\n",
        "    self.unfold = tf.keras.layers.Reshape((self.patch_size, num_patches, self.projection_dim))\n",
        "    self.fold = tf.keras.layers.Reshape((input_shape[1], input_shape[2], self.projection_dim))\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({\n",
        "            'num_blocks': self.num_blocks,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'strides': self.strides,\n",
        "            'patch_size' : self.patch_size,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], input_shape[1], input_shape[2], self.projection_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "    local_features = self.conv_local(x)\n",
        "    patches = self.unfold(local_features)\n",
        "    global_features = self.transformers(patches)\n",
        "    folded_features = self.fold(global_features)\n",
        "    folded_features = self.conv_folded(folded_features)\n",
        "    local_global_features = tf.concat([x, folded_features], axis=-1)\n",
        "    local_global_features = self.conv_local_global(local_global_features)\n",
        "    return local_global_features\n",
        "\n",
        "def MobileViT(input_shape=None, include_top=True, classes=1000, expansion_ratio = 2.0):\n",
        "\n",
        "    img_input = tf.keras.layers.Input(shape=input_shape) # (None, 256, 256, 3)\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        16, 3, padding=\"same\", strides=(2, 2), activation=tf.nn.swish\n",
        "      )(img_input)  # (None, 128, 128, 16) \n",
        "    x = InvertedRes(16 * expansion_ratio, 16, strides=1)(x) # (None, 128, 128, 16)\n",
        "    x = InvertedRes(16 * expansion_ratio, 24, strides=2)(x) # (None, 64, 64, 24) \n",
        "    x = InvertedRes(24 * expansion_ratio, 24, strides=1)(x) # (None, 64, 64, 24)\n",
        "    x = InvertedRes(24 * expansion_ratio, 24, strides=1)(x) # (None, 64, 64, 24)\n",
        "    x = InvertedRes(24 * expansion_ratio, 48, strides=2)(x) # (None, 32, 32, 48)\n",
        "    x = MobileVitBlock(2, 64, strides=1)(x)                 # (None, 32, 32, 64)\n",
        "    x = InvertedRes(64 * expansion_ratio, 64, strides=2)(x) # (None, 16, 16, 64)\n",
        "    x = MobileVitBlock(4, 80, strides=1)(x)                 # (None, 16, 16, 80)\n",
        "    x = InvertedRes(80 * expansion_ratio, 80, strides=2)(x) # (None, 8, 8, 80)\n",
        "    x = MobileVitBlock(3, 96, strides=1)(x)                 # (None, 8, 8, 96)\n",
        "\n",
        "    if include_top:\n",
        "        x = tf.keras.layers.Conv2D(320, 1, padding=\"same\", strides=(1, 1), activation=tf.nn.swish)(x)\n",
        "        x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "        x = tf.keras.layers.Dense(classes, activation=\"sigmoid\")(x)\n",
        "        \n",
        "    # Create model.\n",
        "    model = tf.keras.models.Model(img_input, x)#, name='MobileViT')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIaOPLs__jq6",
        "outputId": "c805d999-8d02-4642-fac1-8de45390cbad"
      },
      "outputs": [],
      "source": [
        "# MobileViT model check\n",
        "\n",
        "#model = MobileViT((256, 256, 3), include_top=True, classes=1)\n",
        "#model.build((None, 256, 256, 3))\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5MRVrzrV2BC"
      },
      "source": [
        "# Model (Temporal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9pcBc0eV1eY"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    \"\"\"Transformer Encoder Block\"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.Dense(dense_dim, activation=tf.nn.gelu), \n",
        "                tf.keras.layers.Dense(embed_dim),\n",
        "             ]\n",
        "        )\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'dense_dim': self.dense_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "        })\n",
        "        return config\n",
        "        \n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSkqvv2gWMI_"
      },
      "outputs": [],
      "source": [
        "def TransformerModel(embed_dim, dense_dim=512, num_heads=4, num_classes=4):\n",
        "  inputs = tf.keras.Input(shape=(512, embed_dim))\n",
        "  x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(inputs)\n",
        "  x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ljqw_h3SIYv",
        "outputId": "1d3f7b8c-4e9e-4424-bc34-c112acdd02ae"
      },
      "outputs": [],
      "source": [
        "# Transformer Encoder model check\n",
        "\n",
        "#temporal_model = TransformerModel(embed_dim=512)\n",
        "#temporal_model.build((None, 512, 512))\n",
        "#temporal_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rqWOylFbUzx"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT5rRfpkMp8v"
      },
      "outputs": [],
      "source": [
        "def create_model(encoder_name, temporal_model_name='lstm', many2one=True):\n",
        "\n",
        "  temporal_input = tf.keras.layers.Input(shape=(SEQ_LEN, IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS))\n",
        "  \n",
        "  # Encoder model\n",
        "  if encoder_name == 'mobilenetv2':   # MobileNetV2 model\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "      input_tensor=tf.keras.layers.Input(shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS)),\n",
        "      input_shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS),\n",
        "      include_top=False,\n",
        "      weights='imagenet'\n",
        "    )\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "\n",
        "  elif encoder_name == 'vgg':   # VGG 19 model\n",
        "    base_model = tf.keras.applications.vgg19.VGG19(\n",
        "        input_shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "\n",
        "  elif encoder_name == 'efficient_b0':    # EfficientNet b0 model\n",
        "    base_model = tf.keras.applications.EfficientNetB0(\n",
        "        input_shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS),\n",
        "        include_top=False, \n",
        "        weights='imagenet'\n",
        "    )\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "\n",
        "  elif encoder_name == 'mobilevit':     # MobileViT model\n",
        "    base_model = MobileViT(\n",
        "      input_shape=(IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS),\n",
        "      include_top=False\n",
        "    )\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "\n",
        "  elif encoder_name == 'vit':     # Visual Transformer base 16 model\n",
        "    # https://github.com/faustomorales/vit-keras/blob/28815edc5c24492612af726d1b2ca78295128d84/vit_keras/vit.py\n",
        "    base_model = vit_b16(\n",
        "      image_size=IMAGE_CROP_SIZE,\n",
        "      activation='sigmoid',\n",
        "      pretrained=True,\n",
        "      include_top=False,\n",
        "      pretrained_top=False,\n",
        "    )\n",
        "    x = base_model.layers[-2].output\n",
        "    x = SliceClassToken()(x) \n",
        "    patch_size = int(math.sqrt(x.shape[-2]))  \n",
        "    embedding_dim = x.shape[-1]\n",
        "    embedding = tf.keras.layers.Reshape((\n",
        "      patch_size, patch_size, \n",
        "      embedding_dim))(x)\n",
        "    base_model = tf.keras.Model(inputs=base_model.input, outputs=embedding)\n",
        "    encoder_output_shape = base_model.output_shape\n",
        "\n",
        "  elif encoder_name == 'movinet': # MoViNet model \n",
        "    # downloading pretrained model checkpoints\n",
        "    !wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a3_base.tar.gz -O movinet_a3_base.tar.gz -q\n",
        "    !tar -xvf movinet_a3_base.tar.gz\n",
        "\n",
        "    checkpoint_dir = 'movinet_a3_base'\n",
        "    checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    checkpoint = tf.train.Checkpoint(model=model)\n",
        "    status = checkpoint.restore(checkpoint_path)\n",
        "    status.assert_existing_objects_matched()\n",
        "\n",
        "    # creating backbone model\n",
        "    model_id = 'a3'\n",
        "    backbone = movinet.Movinet(model_id=model_id)\n",
        "    if many2one:\n",
        "      model = movinet_model.MovinetClassifier(\n",
        "        backbone=backbone,\n",
        "        num_classes=4)\n",
        "      return model\n",
        "\n",
        "    else:\n",
        "      model_encoder = movinet_model.MovinetClassifier(\n",
        "        backbone=backbone,\n",
        "        num_classes= 4 * SEQ_LEN)\n",
        "    \n",
        "      temporal_reshape = tf.keras.layers.Reshape(\n",
        "        (SEQ_LEN, 4)\n",
        "      )\n",
        "      model = tf.keras.Sequential([\n",
        "          model_encoder, \n",
        "          temporal_reshape, \n",
        "      ])\n",
        "      return model\n",
        "      \n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "  # Number of layers to freeze\n",
        "  fine_tune_at = FINE_TUNE_BUFFER\n",
        "  for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "  # Intermidiary layers to match shapes of encoder outputs and temporal model inputs\n",
        "  base_model = tf.keras.layers.TimeDistributed(\n",
        "      base_model, \n",
        "      input_shape=(SEQ_LEN, IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS))\n",
        "\n",
        "  global_avg_pool = tf.keras.layers.TimeDistributed(\n",
        "      tf.keras.layers.GlobalAveragePooling2D(),\n",
        "      input_shape=(8, 8, 96))\n",
        "  flatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\n",
        "  dropout = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "\n",
        "  # temporal model\n",
        "  if temporal_model_name == 'lstm':   # LSTM temporal model\n",
        "    temporal_layer = tf.keras.layers.LSTM(\n",
        "        16, return_sequences= not many2one, dropout=0.2, use_bias=False)\n",
        "\n",
        "  elif temporal_model_name == 'transformer':  # Transformer Encoder temporal model\n",
        "    temporal_layer = TransformerEncoder(\n",
        "        encoder_output_shape[-1], dense_dim=512, num_heads=5, \n",
        "        name=\"transformer_layer\"\n",
        "    )\n",
        "    if many2one == True:   # many 2 one classification head of Transformer model\n",
        "      temporal_reshape = tf.keras.layers.Reshape(\n",
        "        (SEQ_LEN * encoder_output_shape[-1], )\n",
        "      )\n",
        "      linear = tf.keras.layers.Dense(128, activation='relu')\n",
        "      temporal_layer = tf.keras.Sequential([\n",
        "          temporal_layer, \n",
        "          temporal_reshape, \n",
        "          linear\n",
        "      ])\n",
        "\n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "  # output layer\n",
        "  dense = tf.keras.layers.Dense(4, activation='softmax')\n",
        "\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      temporal_input,\n",
        "      base_model,\n",
        "      global_avg_pool,\n",
        "      flatten,\n",
        "      dropout,\n",
        "      temporal_layer,\n",
        "      dense\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
        "\n",
        "def get_flops(model, write_path=None):\n",
        "    concrete = tf.function(lambda inputs: model(inputs))\n",
        "    concrete_func = concrete.get_concrete_function(\n",
        "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
        "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
        "    with tf.Graph().as_default() as graph:\n",
        "        tf.graph_util.import_graph_def(graph_def, name='')\n",
        "        run_meta = tf.compat.v1.RunMetadata()\n",
        "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "        if write_path:\n",
        "            opts['output'] = 'file:outfile={}'.format(write_path)  # suppress output\n",
        "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
        "        return flops.total_float_ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "15ELwcP__qxb",
        "outputId": "629367eb-97df-4f83-a1bc-c293b7a46767"
      },
      "outputs": [],
      "source": [
        "lr = tf.keras.experimental.CosineDecay(BASE_LR, STEPS_PER_EPOCH * EPOCHS) # cosine learning decay\n",
        "\n",
        "with strategy.scope():\n",
        "  print(ENCODER_NAME, TEMPORAL_MODEL)\n",
        "  model = create_model(ENCODER_NAME, TEMPORAL_MODEL, MANY2ONE)\n",
        "  if OPTIMIZER == 'adam':\n",
        "    optimizer = tf.keras.optimizers.Adam(lr) # adam optimizer\n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "      \n",
        "  model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), #'sparse_categorical_crossentropy',\n",
        "    # predict True positives/total images\n",
        "    metrics=['accuracy'],\n",
        "    # NEW on TPU in TensorFlow 24: sending multiple batches to the TPU at once saves communications\n",
        "    # overheads and allows the XLA compiler to unroll the loop on TPU and optimize hardware utilization.\n",
        "    steps_per_execution=16\n",
        "  )\n",
        "\n",
        "model.build((None, SEQ_LEN, IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS))\n",
        "model.summary()\n",
        "parameters = model.count_params()\n",
        "\n",
        "print('Number of parameters: ', parameters)\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "print('FLOPS :', get_flops(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Frames per second measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5CkVlHtS9Ce"
      },
      "outputs": [],
      "source": [
        "# Frames Per Second \n",
        "if EVALUATE_INFERENCE_SPEED:\n",
        "  x = tf.ones((1, SEQ_LEN, IMAGE_CROP_SIZE, IMAGE_CROP_SIZE, NUM_CHANNELS))\n",
        "  print(x.shape)\n",
        "  start_time = time.time()\n",
        "  for i in trange(100):\n",
        "    y = model(x)\n",
        "  elapsed = time.time() - start_time\n",
        "  print('FPS: ', 100 / elapsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY1tNspoq1n8"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOUwn-efPoPn"
      },
      "outputs": [],
      "source": [
        "print(\"Loading... Please wait for data to shuffle\")\n",
        "start_time = time.time()\n",
        "history = model.fit(\n",
        "    get_train_dataset_with_oversample(True), # get_train_dataset(), \n",
        "    steps_per_epoch=STEPS_PER_EPOCH, \n",
        "    epochs=5,\n",
        "    validation_data=get_validation_dataset(repeated=True), \n",
        "    validation_steps=VALIDATION_STEPS,\n",
        "    use_multiprocessing=True\n",
        ")\n",
        "print('Elasped time: ', time.time() - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw3dZAOEt7_y"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/drive/MyDrive/CV_Research/StairNet/Plots/', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/CV_Research/StairNet/CSV/', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUvbH4y-oyXR"
      },
      "source": [
        "## Saving a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMhJr0HXxmUG"
      },
      "outputs": [],
      "source": [
        "if ENCODER_NAME == 'movinet':\n",
        "  model_location = f\"/content/drive/MyDrive/CV_Research/StairNet/Models/{ENCODER_NAME}_Many2one:{MANY2ONE}\"\n",
        "else:\n",
        "  model_location = f\"/content/drive/MyDrive/CV_Research/StairNet/Models/{ENCODER_NAME}_{TEMPORAL_MODEL}_Many2one:{MANY2ONE}\"\n",
        "\n",
        "save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "model.save(model_location, options=save_locally)\n",
        "model.save_weights(model_location + '_weights.h5')\n",
        "#model.save(model_location, save_format='h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAXy5n_FyHBx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (10, 5)\n",
        "data = pd.DataFrame(history.history)\n",
        "metrics = ['loss', 'accuracy']\n",
        "\n",
        "metrics_array = []\n",
        "\n",
        "for metric in metrics:\n",
        "    data[[f'{metric}',f'val_{metric}']].plot()\n",
        "\n",
        "plot_location = f\"/content/drive/MyDrive/CV_Research/StairNet/Plots/{ENCODER_NAME}_{TEMPORAL_MODEL}_Many2one:{MANY2ONE}.jpg\"\n",
        "plt.savefig(plot_location)\n",
        "\n",
        "csv_location = f\"/content/drive/MyDrive/CV_Research/StairNet/CSV/{ENCODER_NAME}_{TEMPORAL_MODEL}_Many2one:{MANY2ONE}.csv\"\n",
        "\n",
        "results_df = pd.DataFrame(history.history)\n",
        "print(results_df)\n",
        "results_df.to_csv(csv_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LFp9OP9o2hh"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUhM5xGPqQAn"
      },
      "outputs": [],
      "source": [
        "cmdataset = get_validation_dataset(distributed=False) # since we are splitting the dataset and iterating separately on images and labels, order matters.\n",
        "labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n",
        "labels_ds = next(iter(labels_ds.batch(NUM_TEST_IMAGES))).numpy() # get everything as one batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_ds, labels_ds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ukhkvBukQpk"
      },
      "outputs": [],
      "source": [
        "if ENCODER_NAME == 'movinet':\n",
        "  model_location = f\"/content/drive/MyDrive/CV_Research/StairNet/Models/{ENCODER_NAME}_Many2one:{MANY2ONE}\"\n",
        "else:\n",
        "  model_location = f\"/content/drive/MyDrive/CV_Research/StairNet/Models/{ENCODER_NAME}_{TEMPORAL_MODEL}_Many2one:{MANY2ONE}\"\n",
        "\n",
        "load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "model = tf.keras.models.load_model('./model_total', options=load_locally)\n",
        "\n",
        "#model = tf.keras.models.load_model(model_location, custom_objects={'TransformerEncoder': TransformerEncoder})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TQDMPredictCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, custom_tqdm_instance=None, tqdm_cls=tqdm, **tqdm_params):\n",
        "        super().__init__()\n",
        "        self.tqdm_cls = tqdm_cls\n",
        "        self.tqdm_progress = None\n",
        "        self.prev_predict_batch = None\n",
        "        self.custom_tqdm_instance = custom_tqdm_instance\n",
        "        self.tqdm_params = tqdm_params\n",
        "\n",
        "    def on_predict_batch_begin(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_predict_batch_end(self, batch, logs=None):\n",
        "        self.tqdm_progress.update(batch - self.prev_predict_batch)\n",
        "        self.prev_predict_batch = batch\n",
        "\n",
        "    def on_predict_begin(self, logs=None):\n",
        "        self.prev_predict_batch = 0\n",
        "        if self.custom_tqdm_instance:\n",
        "            self.tqdm_progress = self.custom_tqdm_instance\n",
        "            return\n",
        "\n",
        "        total = self.params.get('steps')\n",
        "        if total:\n",
        "            total -= 1\n",
        "\n",
        "        self.tqdm_progress = self.tqdm_cls(total=total, **self.tqdm_params)\n",
        "\n",
        "    def on_predict_end(self, logs=None):\n",
        "        if self.tqdm_progress and not self.custom_tqdm_instance:\n",
        "            self.tqdm_progress.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC62haqLd3Bv"
      },
      "outputs": [],
      "source": [
        "tqdm_callback = TQDMPredictCallback()\n",
        "\n",
        "preds = model.predict(cmdataset, callbacks=[tqdm_callback], steps=VALIDATION_STEPS)\n",
        "preds = tf.math.argmax(preds, -1).numpy()\n",
        "\n",
        "print(\"Correct   labels: \", labels_ds.shape, labels_ds)\n",
        "print(\"Predicted labels: \", preds.shape, preds)\n",
        "assert labels_ds.shape == preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxSZj-UZyn-e"
      },
      "outputs": [],
      "source": [
        "def display_confusion_matrix(cmat, accuracy, score, precision, recall, filename):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    ax = plt.gca()\n",
        "    sns.heatmap(cmat, annot=True, fmt='.2g', ax=ax);\n",
        "    ax.set_xticks(range(len(CLASS_MAP)))\n",
        "    ax.set_xticklabels(CLASS_MAP, fontdict={'fontsize': 7})\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
        "    ax.set_yticks(range(len(CLASS_MAP)))\n",
        "    ax.set_yticklabels(CLASS_MAP, fontdict={'fontsize': 7})\n",
        "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    titlestring = \"\"\n",
        "    if score is not None:\n",
        "        titlestring += 'F1 = {:.3f} '.format(score)\n",
        "    if accuracy is not None:\n",
        "        titlestring += 'Accuracy = {:.3f} '.format(accuracy)\n",
        "    if precision is not None:\n",
        "        titlestring += '\\nPrecision = {:.3f} '.format(precision)\n",
        "    if recall is not None:\n",
        "        titlestring += 'Recall = {:.3f} '.format(recall)\n",
        "    if len(titlestring) > 0:\n",
        "        plt.title(titlestring)\n",
        "    plt.savefig(filename)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCWNMg5byboO"
      },
      "outputs": [],
      "source": [
        "filename = f\"/content/drive/MyDrive/CV_Research/StairNet/Plots/{ENCODER_NAME}_{TEMPORAL_MODEL}_Many2one:{MANY2ONE}_confusion_matrix.png\"\n",
        "\n",
        "if ENCODER_NAME == 'movinet':\n",
        "  filename = f\"/content/drive/MyDrive/CV_Research/StairNet/Plots/{ENCODER_NAME}_Many2one:{MANY2ONE}_confusion_matrix.png\"\n",
        "else:\n",
        "  filename = f\"/content/drive/MyDrive/CV_Research/StairNet/Plots/{ENCODER_NAME}_{TEMPORAL_MODEL}_Many2one:{MANY2ONE}_confusion_matrix.png\"\n",
        "\n",
        "cmat = confusion_matrix(labels_ds, preds, labels=range(len(CLASS_MAP)))\n",
        "# score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
        "score = f1_score(labels_ds, preds, labels=range(len(CLASS_MAP)), average='weighted')\n",
        "# precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
        "precision = precision_score(labels_ds, preds, labels=range(len(CLASS_MAP)), average='weighted')\n",
        "# recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
        "recall = recall_score(labels_ds, preds, labels=range(len(CLASS_MAP)), average='weighted')\n",
        "cmat = (cmat.T / cmat.sum(axis=1)).T # normalized\n",
        "accuracy = accuracy_score(labels_ds, preds)\n",
        "display_confusion_matrix(cmat, accuracy, score, precision, recall, filename)\n",
        "print('accuracy: {:.5f}, f1 score: {:.5f}, precision: {:.5f}, recall: {:.5f}'.format(accuracy, score, precision, recall))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "vid_cl_tf_tpu_pipeline.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
