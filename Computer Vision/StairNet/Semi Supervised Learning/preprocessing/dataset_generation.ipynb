{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MpGo8V3rKoB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjIiospPrJUS"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJcTMgCMlyv2"
      },
      "outputs": [],
      "source": [
        "project_id = 'stairnet'\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olvaemTDl7mu"
      },
      "outputs": [],
      "source": [
        "# Test to see if dataset location is correct\n",
        "! gsutil ls -al gs://stairnet_bucket/StairNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFuf3T43oC70"
      },
      "outputs": [],
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT5Y8q_Fo9bD"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!gcsfuse --implicit-dirs stairnet_bucket data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA-bDR-0rPOB"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCb3D5lxnqLz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqLHT1TU4Y1n"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcacLuQapxjU"
      },
      "outputs": [],
      "source": [
        "# Base folder name\n",
        "FOLDER_NAME = 'data/StairNet'\n",
        "\n",
        "# Created folder name\n",
        "SEQUENCE_FOLDER = 'drive/MyDrive/StairNet/Sample_Sequences/'\n",
        "\n",
        "# Names of the classification classes\n",
        "CLASS_NAMES = ['IS', 'ISLG', 'LG', 'LGIS']\n",
        "\n",
        "# Number of frames in each sequence\n",
        "SEQ_SIZE = 5\n",
        "\n",
        "# Type of sequences, when the frame is the first in the video\n",
        "PADDING_TYPE = 'copy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adErvyAmsWps"
      },
      "outputs": [],
      "source": [
        "def count_files(folder_name):\n",
        "    ''' \n",
        "        Count number of files in the folder \n",
        "    '''\n",
        "    counter = 0\n",
        "    for folder in list(filter(('.DS_Store').__ne__, os.listdir(folder_name))):\n",
        "        curr_count = len(os.listdir(os.path.join(folder_name, folder)))\n",
        "        print('Number of files in folder {}: {}'.format(folder, curr_count))\n",
        "        counter += curr_count\n",
        "    print('Total number of files: ', counter)\n",
        "\n",
        "\n",
        "def get_frame_samples(\n",
        "        class_folder_names,\n",
        "        filter_ds_store=True):\n",
        "    '''\n",
        "        get list of all video frames\n",
        "    '''\n",
        "    samples = list()\n",
        "    for folder in class_folder_names:\n",
        "        for el in os.listdir(folder):\n",
        "            samples.append(el)\n",
        "    if filter_ds_store:\n",
        "        return list(filter(('.DS_Store').__ne__, samples))\n",
        "    return samples\n",
        "\n",
        "def get_video_number(file_name):\n",
        "    '''\n",
        "        parse video number from string\n",
        "        input: [IMG_#_#] frame # #CLASS#.jpg\n",
        "        output: IMG_#_#\n",
        "    '''\n",
        "    return file_name.split(' ')[0].replace(\"['\", '').replace(\"']\", '').replace(\"'\", '')\n",
        "\n",
        "\n",
        "def get_class(filename):\n",
        "    '''\n",
        "        parse frame label from filename\n",
        "        input: [IMG_#_#] frame # #CLASS#.jpg\n",
        "        output: #CLASS#\n",
        "    '''\n",
        "    return filename.split(' ')[-1].split('.jpg')[0]\n",
        "\n",
        "def get_frame_number(filename):\n",
        "    '''\n",
        "        parse frame number from filename\n",
        "        input: [IMG_#_#] frame # #CLASS#.jpg\n",
        "        output: frame #\n",
        "    '''\n",
        "    return int(filename.split (' ')[2])\n",
        "\n",
        "\n",
        "def create_video2frame_dict(video_samples, data_folder, log=False):\n",
        "    ''' mapping video number to corresponding frames '''\n",
        "    video_names_dict = {get_video_number(el): list() for el in video_samples}\n",
        "\n",
        "    # creating folder for each video\n",
        "    for name in video_names_dict.keys():\n",
        "        dir_name = os.path.join(data_folder, name)\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        if log:\n",
        "            print(f'\\t Created video folder: {dir_name}')\n",
        "\n",
        "    # mapping video with corresponding sorted frames\n",
        "    for image_path in sorted(\n",
        "            video_samples, key=lambda x: int(\n",
        "                get_frame_number(x))):\n",
        "        video_n = get_video_number(image_path)\n",
        "        video_names_dict[video_n].append(image_path)\n",
        "\n",
        "    return video_names_dict\n",
        "\n",
        "def get_video_episodes(video_seq):\n",
        "    ''' \n",
        "        Splitting video sequence by episodes\n",
        "    '''\n",
        "    episodes = list()\n",
        "    curr_episode = list()\n",
        "    counter = 0\n",
        "    frame_idx = 0\n",
        "    while counter < get_frame_number(video_seq[-1]):\n",
        "        if counter == get_frame_number(video_seq[frame_idx]):\n",
        "            curr_episode.append(video_seq[frame_idx])\n",
        "            frame_idx += 1\n",
        "        else:\n",
        "            episodes.append(curr_episode)\n",
        "            curr_episode = list()\n",
        "        counter += 6\n",
        "    episodes.append(curr_episode)\n",
        "    return [x for x in episodes if x != []]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYXASZbJsaUR"
      },
      "outputs": [],
      "source": [
        "count_files(FOLDER_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmLS-Ca5sbOs"
      },
      "outputs": [],
      "source": [
        "class_folder_names = [\n",
        "    os.path.join(FOLDER_NAME, class_name) for class_name in CLASS_NAMES\n",
        "]\n",
        "print('Class folders:\\n', class_folder_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pElkUihvtQP4"
      },
      "outputs": [],
      "source": [
        "samples = get_frame_samples(\n",
        "    class_folder_names=class_folder_names\n",
        ")\n",
        "print('Number of samples: ', len(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Qn1zDatSq5"
      },
      "outputs": [],
      "source": [
        "video_dict = create_video2frame_dict(\n",
        "    video_samples=samples, \n",
        "    data_folder=SEQUENCE_FOLDER\n",
        ")\n",
        "print('Number of videos: ', len(video_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUPr_DnQtU35"
      },
      "outputs": [],
      "source": [
        "video_names = video_dict.keys()\n",
        "video_length = [len(video_dict[name]) for name in video_names]\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "plt.title('Number of frames in each video')\n",
        "x = np.arange(len(video_names))\n",
        "plt.bar(x, height=video_length)\n",
        "plt.xticks(x, video_names, rotation = 45);\n",
        "plt.savefig('video_len_distribution.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tCMatpTxl8d"
      },
      "outputs": [],
      "source": [
        "np.min(video_length), np.max(video_length), np.mean(video_length), np.std(video_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWZHp2aYtWZx"
      },
      "outputs": [],
      "source": [
        "episode_dict = {key: get_video_episodes(value) for key, value in video_dict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuXzE7VJtYCA"
      },
      "outputs": [],
      "source": [
        "video_names = episode_dict.keys()\n",
        "episode_num = [len(episode_dict[name]) for name in video_names]\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "plt.title('Number of episodes in each video')\n",
        "x = np.arange(len(video_names))\n",
        "plt.bar(x, height=episode_num)\n",
        "plt.xticks(x, video_names, rotation = 45);\n",
        "plt.savefig('video_episode_num.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlbtzH2ByklX"
      },
      "outputs": [],
      "source": [
        "np.min(episode_num), np.max(episode_num), np.mean(episode_num), np.std(episode_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLUfYkW-7gcA"
      },
      "outputs": [],
      "source": [
        "episode_names = list()\n",
        "episode_len = list()\n",
        "\n",
        "for vid_name in tqdm(episode_dict.keys()):\n",
        "  for e_idx, episode in enumerate(episode_dict[vid_name]):\n",
        "    episode_names.append(f'{vid_name}_episode_{e_idx}')\n",
        "    episode_len.append(len(episode))\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "plt.title('Number of frames in each episodes')\n",
        "x = np.arange(len(episode_names))\n",
        "plt.bar(x, height=episode_len)\n",
        "#plt.xticks(x, episode_names, rotation = 45);\n",
        "plt.xticks([])\n",
        "plt.savefig('episode_len_distribution.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmyM-VeByy8Q"
      },
      "outputs": [],
      "source": [
        "np.min(episode_len), np.max(episode_len), np.mean(episode_len), np.std(episode_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-LJ5WFsta83"
      },
      "outputs": [],
      "source": [
        "def construct_samples(video, seq_len):\n",
        "    samples = list()\n",
        "    if len(video) < seq_len:\n",
        "        for i in range(len(video)):\n",
        "            subset = [video[0]] * (seq_len - i - 1) + video[:i + 1]\n",
        "            samples.append(subset)\n",
        "\n",
        "    else:\n",
        "        for i in range(0, len(video)):\n",
        "            subset = [video[0]] * (seq_len - i - 1) + video[:i + 1]\n",
        "            samples.append(subset[-seq_len:])\n",
        "    return samples\n",
        "\n",
        "def generate_seq_dataset(video_dict, data_folder, save_folder, seq_size):\n",
        "    # iterate over video in dataset\n",
        "    for video_name, episodes in tqdm(video_dict.items()):\n",
        "        counter = 0\n",
        "        # for each episode in video construct samples of length `seq_size`\n",
        "        for episode in episodes: \n",
        "            seq_samples = construct_samples(episode, seq_len=seq_size)\n",
        "\n",
        "            # save samples\n",
        "            for sample in seq_samples:\n",
        "                with open(os.path.join(save_folder, video_name, f'sample_{counter}.txt'), 'rw') as f:\n",
        "                    f.write(\n",
        "                        '\\n'.join(\n",
        "                            [os.path.join(data_folder, get_class(el), el) for el in sample]\n",
        "                        )\n",
        "                    )\n",
        "                counter += 1\n",
        "    return counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L5SvucOtdJX"
      },
      "outputs": [],
      "source": [
        " num_files = generate_seq_dataset(\n",
        "    video_dict=episode_dict,\n",
        "    data_folder=FOLDER_NAME,\n",
        "    save_folder=SEQUENCE_FOLDER,\n",
        "    seq_size=SEQ_SIZE\n",
        ")\n",
        "print('Number of files: ', num_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcMNg5jDuFtn"
      },
      "source": [
        "# Train, Val and Test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncaj-Z-atfFK"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\n",
        "    '/content/data/StairNet_Split_CSV/StairNet_Train.csv')\n",
        "val = pd.read_csv(\n",
        "    '/content/data/StairNet_Split_CSV/StairNet_Validation.csv')\n",
        "test = pd.read_csv(\n",
        "    '/content/data/StairNet_Split_CSV/StairNet_Test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuST9ILZujnL"
      },
      "outputs": [],
      "source": [
        "train.shape, val.shape, test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbkwd96tu0CU"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewH22cFvu1Ph"
      },
      "outputs": [],
      "source": [
        "train_frames = train['filename'].values\n",
        "val_frames = val['filename'].values\n",
        "test_frames = test['filename'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq9GstT8u2rx"
      },
      "outputs": [],
      "source": [
        "TRAIN_FOLDER = 'drive/MyDrive/StairNet/Splits/Train'\n",
        "VAL_FOLDER = 'drive/MyDrive/StairNet/Splits/Val'\n",
        "TEST_FOLDER = 'drive/MyDrive/StairNet/Splits/Test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQG6Ykjzu4Fc"
      },
      "outputs": [],
      "source": [
        "os.makedirs(TRAIN_FOLDER, exist_ok=True)\n",
        "os.makedirs(VAL_FOLDER, exist_ok=True)\n",
        "os.makedirs(TEST_FOLDER, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIsp_1luu5lg"
      },
      "outputs": [],
      "source": [
        "def get_frame_name(filename):\n",
        "    return filename.split('/')[-1]\n",
        "\n",
        "def save_file(new_path, data):\n",
        "    curr_folder = '/'.join(new_path.split('/')[:-1])\n",
        "    if not os.path.exists(curr_folder):\n",
        "      os.makedirs(curr_folder)\n",
        "    with open(new_path, 'w') as f:\n",
        "        f.write(\n",
        "            '\\n'.join(data)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qGBIzRAu9VJ"
      },
      "outputs": [],
      "source": [
        "samples = list()\n",
        "for video_name in list(filter(('.DS_Store').__ne__, os.listdir('drive/MyDrive/StairNet/Sequences/'))):\n",
        "    for seq in tqdm(os.listdir(os.path.join('drive/MyDrive/StairNet/Sequences/', video_name))):\n",
        "        if seq == 'frames.txt':\n",
        "            continue\n",
        "        curr_filename = os.path.join('drive/MyDrive/StairNet/Sequences/', video_name, seq)\n",
        "        f = open(curr_filename, 'r')\n",
        "        data = f.read().splitlines()\n",
        "        f.close()\n",
        "        curr_sample = get_frame_name(data[-1])\n",
        "        if curr_sample in train_frames:\n",
        "            save_file(os.path.join(TRAIN_FOLDER, video_name, seq), data)\n",
        "        elif curr_sample in val_frames:\n",
        "            save_file(os.path.join(VAL_FOLDER, video_name, seq), data)\n",
        "        elif curr_sample in test_frames:\n",
        "            save_file(os.path.join(TEST_FOLDER, video_name, seq), data)\n",
        "        else:\n",
        "            print(curr_filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXV9TKfCpdAU"
      },
      "source": [
        "# (Optional) Numpy sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCy04aFtqNrj"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "import multiprocessing\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCFrStkDpgPR"
      },
      "outputs": [],
      "source": [
        "# Base folder name\n",
        "IMG_FOLDER = 'data/StairNet/'\n",
        "\n",
        "#SAMPLE_SPLIT_TRAIN = 'data/StairNet_Seq_5/Splits_Random/Train'\n",
        "#SAMPLE_SPLIT_VAL = 'data/StairNet_Seq_5/Splits_Random/Val'\n",
        "#SAMPLE_SPLIT_TEST = 'data/StairNet_Seq_5/Splits_Random/Test'\n",
        "\n",
        "# Folder name to save files\n",
        "SAMPLE_SEQUENCES = '/content/data/StairNet_Seq_5/Sample_Sequences'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91qqGKGz7AmS"
      },
      "outputs": [],
      "source": [
        "#os.makedirs('data/StairNet_Seq_5/SplitsVideo_numpy/Train')\n",
        "#os.makedirs('data/StairNet_Seq_5/SplitsVideo_numpy/Val')\n",
        "#os.makedirs('data/StairNet_Seq_5/SplitsVideo_numpy/Test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXspF7ozrEZh"
      },
      "outputs": [],
      "source": [
        "def load_image(filename: str, img_load: str ='pil', img_size=(256, 256)):\n",
        "  ''' Loading image as ndarray '''\n",
        "  if img_load == 'cv2':\n",
        "    img = cv2.imread(filename)\n",
        "    img.resize(img_size)\n",
        "  elif img_load == 'pil':\n",
        "    img = Image.open(filename)\n",
        "    img = img.resize(img_size, Image.ANTIALIAS)  \n",
        "  return np.array(img)\n",
        "\n",
        "def read_sample(sample_seq, img_size):\n",
        "  ''' Loading sequence of frames to one ndarray '''\n",
        "  buffer = np.zeros(shape=(len(sample_seq), img_size, img_size, 3))\n",
        "  labels = list()\n",
        "  for idx, img_path in enumerate(sample_seq):\n",
        "      buffer[idx] = load_image(img_path, img_load='pil')\n",
        "      labels.append(img_path.split('/')[-1].split(' ')[-1].split('.')[0])\n",
        "  return buffer, labels\n",
        "\n",
        "def read_seq_file(filename):\n",
        "  ''' reading file with samples '''\n",
        "  with open(filename, \"r\") as f:\n",
        "    data = f.readlines()\n",
        "  data = [d.strip() for d in data]\n",
        "  return data\n",
        "\n",
        "def save_seq(arr, labels, idx, folder_name):\n",
        "  ''' Saving np ndarray as .npy '''\n",
        "  label = '_'.join([el.split('/')[-1].split(' ')[-1].split('.')[0] for el in labels])\n",
        "  filename = folder_name + f'/idx_{idx}:' + label + '.npy'\n",
        "  np.save(filename, arr)\n",
        "\n",
        "def sample_processing(sample):\n",
        "  ''' processing samples by reading the frames and constructing sequnce ndarray '''\n",
        "  filename, idx, folder_name = sample\n",
        "  seq_path_arr = read_seq_file(filename)\n",
        "  frames, lables = read_sample(seq_path_arr, 256)\n",
        "  save_seq(frames, lables, idx, folder_name) \n",
        "  return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk0as7rIweSY"
      },
      "outputs": [],
      "source": [
        "video_list = [\n",
        "'IMG_02_1', 'IMG_02_4', 'IMG_05_1', 'IMG_11_1', 'IMG_14_2', 'IMG_20_1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFhv6C0468p_"
      },
      "outputs": [],
      "source": [
        "def process_data_split(folder_name, save_folder_name, video_list=None):\n",
        "  ''' Generate sequence samples for provided data split '''\n",
        "  samples = list()\n",
        "  counter = 0\n",
        "  for video_name in tqdm(video_list): # tqdm(list(filter(('.DS_Store').__ne__, os.listdir(folder_name)))):\n",
        "    for sample in os.listdir(os.path.join(folder_name, video_name)):\n",
        "      samples.append(\n",
        "          (os.path.join(folder_name, video_name, sample), counter, save_folder_name)\n",
        "      )\n",
        "      counter += 1\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDzSXufK8CJP"
      },
      "outputs": [],
      "source": [
        "samples_val = process_data_split(SAMPLE_SEQUENCES, 'data/StairNet_Seq_5/SplitsVideo_numpy/Test', video_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhHCdx9J-IPo"
      },
      "outputs": [],
      "source": [
        "with multiprocessing.Pool(processes = 16) as p:\n",
        "    res = list(tqdm(p.imap(sample_processing, samples_val), total=len(samples_val)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uA-bDR-0rPOB",
        "hcMNg5jDuFtn"
      ],
      "name": "Video_dataset_preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
